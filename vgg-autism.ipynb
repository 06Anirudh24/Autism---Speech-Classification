{"metadata":{"accelerator":"TPU","colab":{"provenance":[]},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7405399,"sourceType":"datasetVersion","datasetId":4306525},{"sourceId":8588058,"sourceType":"datasetVersion","datasetId":5136800}],"dockerImageVersionId":30716,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"papermill":{"default_parameters":{},"duration":3140.953392,"end_time":"2024-01-16T14:19:46.590048","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-01-16T13:27:25.636656","version":"2.4.0"}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**Dataset Information: -**\n\n\n1.   https://www.spectrumnews.org/news/researchers-publish-new-dataset-on-minimally-verbal-autistic-people/\n2.   https://zenodo.org/record/5786860#.ZG0w_HZBw2w\n\n","metadata":{"id":"a1d703f6"}},{"cell_type":"markdown","source":"**Installing files from Zenodo: -**\n\n\n1.   pip install zenodo-get\n2.   zenodo_get 10.5281/zenodo.5786860\n\n(10.5281/zenodo.5786860 is the DOI of the database publication)\n\n\n","metadata":{"id":"67ce48a7"}},{"cell_type":"markdown","source":"#Imports","metadata":{"id":"b4d8838a"}},{"cell_type":"code","source":"#%tensorflow_version 2.x  # this line is not required unless you are in a notebook\nimport tensorflow as tf\nfrom tensorflow.keras import datasets, layers, models\nimport matplotlib.pyplot as plt","metadata":{"id":"c325f3ae","execution":{"iopub.status.busy":"2024-06-04T16:30:59.432453Z","iopub.execute_input":"2024-06-04T16:30:59.432985Z","iopub.status.idle":"2024-06-04T16:31:11.254345Z","shell.execute_reply.started":"2024-06-04T16:30:59.432952Z","shell.execute_reply":"2024-06-04T16:31:11.249553Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"2024-06-04 16:31:01.200847: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-06-04 16:31:01.200950: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-06-04 16:31:01.330201: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\nimport math, random\nimport torch\nimport torchaudio\nfrom torchaudio import transforms\nfrom IPython.display import Audio","metadata":{"id":"ccb563f7","execution":{"iopub.status.busy":"2024-06-04T16:31:11.256084Z","iopub.execute_input":"2024-06-04T16:31:11.256649Z","iopub.status.idle":"2024-06-04T16:31:11.261917Z","shell.execute_reply.started":"2024-06-04T16:31:11.256621Z","shell.execute_reply":"2024-06-04T16:31:11.260814Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import DataLoader, Dataset, random_split\nimport torchaudio\nimport torch.nn.functional as F\nfrom torch.nn import init\nimport torch\nimport torchvision\nimport torch.nn as nn\n\nimport torchvision.models as models","metadata":{"id":"83a5897e","execution":{"iopub.status.busy":"2024-06-04T16:31:11.263301Z","iopub.execute_input":"2024-06-04T16:31:11.267286Z","iopub.status.idle":"2024-06-04T16:31:12.878715Z","shell.execute_reply.started":"2024-06-04T16:31:11.267248Z","shell.execute_reply":"2024-06-04T16:31:12.877734Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"#Read and Prepare Dataset\n","metadata":{"id":"5c5a95ee"}},{"cell_type":"code","source":"path = \"/kaggle/input/autism-paper-data/Retained/dataset_file_directory.csv\"\ndf_original = pd.read_csv(path)\ndf_original.head()\n\n#There are 6 classes of sounds in the dataset.\n#The class label is categorical, and hence will be converted to a numeric Class ID later.\n#E.g.: 0 = air conditioner, 1 = car horn, etc.","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":224},"id":"057cc445","outputId":"9ee00de7-0e20-4d2e-c6bb-2740f5af57fc","execution":{"iopub.status.busy":"2024-06-04T16:31:12.880806Z","iopub.execute_input":"2024-06-04T16:31:12.881104Z","iopub.status.idle":"2024-06-04T16:31:12.943914Z","shell.execute_reply.started":"2024-06-04T16:31:12.881078Z","shell.execute_reply":"2024-06-04T16:31:12.942973Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"                                    Filename Participant   Label\n0   210408_2025_00-01-38.27--00-01-40.84.wav         P01  social\n1   210324_2036_00-06-03.61--00-06-05.78.wav         P01  social\n2   210324_2036_00-09-04.66--00-09-06.16.wav         P01  social\n3     210324_2036_00-11-18.6--00-11-20.3.wav         P01  social\n4  200506_2110_00-01-25.92--00-01-26.58c.wav         P01  social","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Filename</th>\n      <th>Participant</th>\n      <th>Label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>210408_2025_00-01-38.27--00-01-40.84.wav</td>\n      <td>P01</td>\n      <td>social</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>210324_2036_00-06-03.61--00-06-05.78.wav</td>\n      <td>P01</td>\n      <td>social</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>210324_2036_00-09-04.66--00-09-06.16.wav</td>\n      <td>P01</td>\n      <td>social</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>210324_2036_00-11-18.6--00-11-20.3.wav</td>\n      <td>P01</td>\n      <td>social</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>200506_2110_00-01-25.92--00-01-26.58c.wav</td>\n      <td>P01</td>\n      <td>social</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"Training Data: -\n*   The features (X): Audio Paths\n*   The target labels (y): Class Names","metadata":{"id":"d900f763"}},{"cell_type":"code","source":"df_original.shape[0]","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a3f135f6","outputId":"77ca3142-bf03-4909-cdb0-9a73ffb7c96a","execution":{"iopub.status.busy":"2024-06-04T16:31:12.945006Z","iopub.execute_input":"2024-06-04T16:31:12.945829Z","iopub.status.idle":"2024-06-04T16:31:12.951830Z","shell.execute_reply.started":"2024-06-04T16:31:12.945800Z","shell.execute_reply":"2024-06-04T16:31:12.950865Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"6450"},"metadata":{}}]},{"cell_type":"code","source":"df_original.Label.unique() #Print unique types of sound","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b219feb8","outputId":"5761332e-54a2-4066-9d8c-40862ef9d6cb","execution":{"iopub.status.busy":"2024-06-04T16:31:12.953114Z","iopub.execute_input":"2024-06-04T16:31:12.953462Z","iopub.status.idle":"2024-06-04T16:31:12.964252Z","shell.execute_reply.started":"2024-06-04T16:31:12.953431Z","shell.execute_reply":"2024-06-04T16:31:12.963451Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"array(['social', 'selftalk', 'request', 'delighted', 'dysregulated',\n       'frustrated'], dtype=object)"},"metadata":{}}]},{"cell_type":"code","source":"df_original['Label'].nunique() #Our dataset has 22 target variables/classes.","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cda344cf","outputId":"7af7f16f-d298-4aee-ba8f-ea52c5c3ebdb","execution":{"iopub.status.busy":"2024-06-04T16:31:12.965355Z","iopub.execute_input":"2024-06-04T16:31:12.965624Z","iopub.status.idle":"2024-06-04T16:31:12.975779Z","shell.execute_reply.started":"2024-06-04T16:31:12.965601Z","shell.execute_reply":"2024-06-04T16:31:12.974678Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"6"},"metadata":{}}]},{"cell_type":"code","source":"df_original['Label'].value_counts() #Number of datapoints available for each target variable","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7e40f3af","outputId":"18f52613-cc15-4fec-bf59-0e65f0b0334e","execution":{"iopub.status.busy":"2024-06-04T16:31:12.977060Z","iopub.execute_input":"2024-06-04T16:31:12.977464Z","iopub.status.idle":"2024-06-04T16:31:12.992921Z","shell.execute_reply.started":"2024-06-04T16:31:12.977434Z","shell.execute_reply":"2024-06-04T16:31:12.991922Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"Label\nselftalk        1885\nfrustrated      1536\ndelighted       1272\ndysregulated     704\nsocial           634\nrequest          419\nName: count, dtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"#Idea: Consider only the top 6 majority classes and drop the rest to account for class imbalance.\n#Problem: Path (file) names are random, and they are not foldered/organzied. Hence we need to write a code that will read the csv file after deletion, and pick only the un-deleted ones from the audio folder.\n\"\"\"\n#drop minority classes\nindex_names = df_original[ df_original['Label'] == \"greeting\" ].index\ndf_original.drop(index_names, inplace = True)\n\nindex_names = df_original[ df_original['Label'] == \"hunger\" ].index\ndf_original.drop(index_names, inplace = True)\n\nindex_names = df_original[ df_original['Label'] == \"tablet\" ].index\ndf_original.drop(index_names, inplace = True)\n\nindex_names = df_original[ df_original['Label'] == \"laugh\" ].index\ndf_original.drop(index_names, inplace = True)\n\nindex_names = df_original[ df_original['Label'] == \"glee\" ].index\ndf_original.drop(index_names, inplace = True)\n\nindex_names = df_original[ df_original['Label'] == \"no\" ].index\ndf_original.drop(index_names, inplace = True)\n\nindex_names = df_original[ df_original['Label'] == \"dysregulation-bathroom\" ].index\ndf_original.drop(index_names, inplace = True)\n\nindex_names = df_original[ df_original['Label'] == \"bathroom\" ].index\ndf_original.drop(index_names, inplace = True)\n\nindex_names = df_original[ df_original['Label'] == \"protest\" ].index\ndf_original.drop(index_names, inplace = True)\n\nindex_names = df_original[ df_original['Label'] == \"more\" ].index\ndf_original.drop(index_names, inplace = True)\n\nindex_names = df_original[ df_original['Label'] == \"help\" ].index\ndf_original.drop(index_names, inplace = True)\n\nindex_names = df_original[ df_original['Label'] == \"happy\" ].index\ndf_original.drop(index_names, inplace = True)\n\nindex_names = df_original[ df_original['Label'] == \"dysregulation-sick\" ].index\ndf_original.drop(index_names, inplace = True)\n\nindex_names = df_original[ df_original['Label'] == \"yes\" ].index\ndf_original.drop(index_names, inplace = True)\n\nindex_names = df_original[ df_original['Label'] == \"affectionate\" ].index\ndf_original.drop(index_names, inplace = True)\n\nindex_names = df_original[ df_original['Label'] == \"laughter\" ].index\ndf_original.drop(index_names, inplace = True)\n\"\"\"","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":232},"id":"64af4314","outputId":"8654af3a-68f5-4c5e-9a0d-5d126d18400d","execution":{"iopub.status.busy":"2024-06-04T16:31:12.994093Z","iopub.execute_input":"2024-06-04T16:31:12.994354Z","iopub.status.idle":"2024-06-04T16:31:13.002737Z","shell.execute_reply.started":"2024-06-04T16:31:12.994332Z","shell.execute_reply":"2024-06-04T16:31:13.001846Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"'\\n#drop minority classes\\nindex_names = df_original[ df_original[\\'Label\\'] == \"greeting\" ].index\\ndf_original.drop(index_names, inplace = True)\\n\\nindex_names = df_original[ df_original[\\'Label\\'] == \"hunger\" ].index\\ndf_original.drop(index_names, inplace = True)\\n\\nindex_names = df_original[ df_original[\\'Label\\'] == \"tablet\" ].index\\ndf_original.drop(index_names, inplace = True)\\n\\nindex_names = df_original[ df_original[\\'Label\\'] == \"laugh\" ].index\\ndf_original.drop(index_names, inplace = True)\\n\\nindex_names = df_original[ df_original[\\'Label\\'] == \"glee\" ].index\\ndf_original.drop(index_names, inplace = True)\\n\\nindex_names = df_original[ df_original[\\'Label\\'] == \"no\" ].index\\ndf_original.drop(index_names, inplace = True)\\n\\nindex_names = df_original[ df_original[\\'Label\\'] == \"dysregulation-bathroom\" ].index\\ndf_original.drop(index_names, inplace = True)\\n\\nindex_names = df_original[ df_original[\\'Label\\'] == \"bathroom\" ].index\\ndf_original.drop(index_names, inplace = True)\\n\\nindex_names = df_original[ df_original[\\'Label\\'] == \"protest\" ].index\\ndf_original.drop(index_names, inplace = True)\\n\\nindex_names = df_original[ df_original[\\'Label\\'] == \"more\" ].index\\ndf_original.drop(index_names, inplace = True)\\n\\nindex_names = df_original[ df_original[\\'Label\\'] == \"help\" ].index\\ndf_original.drop(index_names, inplace = True)\\n\\nindex_names = df_original[ df_original[\\'Label\\'] == \"happy\" ].index\\ndf_original.drop(index_names, inplace = True)\\n\\nindex_names = df_original[ df_original[\\'Label\\'] == \"dysregulation-sick\" ].index\\ndf_original.drop(index_names, inplace = True)\\n\\nindex_names = df_original[ df_original[\\'Label\\'] == \"yes\" ].index\\ndf_original.drop(index_names, inplace = True)\\n\\nindex_names = df_original[ df_original[\\'Label\\'] == \"affectionate\" ].index\\ndf_original.drop(index_names, inplace = True)\\n\\nindex_names = df_original[ df_original[\\'Label\\'] == \"laughter\" ].index\\ndf_original.drop(index_names, inplace = True)\\n'"},"metadata":{}}]},{"cell_type":"code","source":"#Convert Text to number notation for target variable (Categorical to numerical data)\n\n\"\"\"\nCombine classes to account for class imbalance and to reduce the number of classes: -\nself talk - 0\nfrusturated, protest, no - 1\ndelighted, laughter, happy, glee, laugh  - 2\ndysregulated, dysregulation-sick, dysregulation-bathroom - 3\nsocial, affectionate, greeting - 4\nrequest, help, bathroom, tablet, bathroom - 5\nyes, more, hunger - 6\n\"\"\"\n\ndf_original[\"classID\"] = df_original['Label']\ndf_original['classID'].replace(['selftalk', 'frustrated', 'delighted', 'dysregulated', 'social', 'request'],\n                        [0, 1, 2, 3, 4, 5], inplace=True)\n\n\n#Create new dataframe called 'df' with only two columns - Filename, classID.\ndf = df_original\ndf = df.drop(['Participant', 'Label'], axis=1)\ndf.head()","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"77916d44","outputId":"9101fec0-1712-40af-dce5-38a7a4e22060","execution":{"iopub.status.busy":"2024-06-04T16:31:13.005983Z","iopub.execute_input":"2024-06-04T16:31:13.006323Z","iopub.status.idle":"2024-06-04T16:31:13.033395Z","shell.execute_reply.started":"2024-06-04T16:31:13.006292Z","shell.execute_reply":"2024-06-04T16:31:13.032533Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_61/4223752362.py:15: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\nThe behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n\nFor example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n\n\n  df_original['classID'].replace(['selftalk', 'frustrated', 'delighted', 'dysregulated', 'social', 'request'],\n/tmp/ipykernel_61/4223752362.py:15: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n  df_original['classID'].replace(['selftalk', 'frustrated', 'delighted', 'dysregulated', 'social', 'request'],\n","output_type":"stream"},{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"                                    Filename  classID\n0   210408_2025_00-01-38.27--00-01-40.84.wav        4\n1   210324_2036_00-06-03.61--00-06-05.78.wav        4\n2   210324_2036_00-09-04.66--00-09-06.16.wav        4\n3     210324_2036_00-11-18.6--00-11-20.3.wav        4\n4  200506_2110_00-01-25.92--00-01-26.58c.wav        4","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Filename</th>\n      <th>classID</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>210408_2025_00-01-38.27--00-01-40.84.wav</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>210324_2036_00-06-03.61--00-06-05.78.wav</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>210324_2036_00-09-04.66--00-09-06.16.wav</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>210324_2036_00-11-18.6--00-11-20.3.wav</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>200506_2110_00-01-25.92--00-01-26.58c.wav</td>\n      <td>4</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"df['classID'].value_counts() #Number of datapoints available for each target variable","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"46eb846d","outputId":"4211e839-069b-4b3a-e27c-ce01f3bff708","execution":{"iopub.status.busy":"2024-06-04T16:31:13.034537Z","iopub.execute_input":"2024-06-04T16:31:13.034893Z","iopub.status.idle":"2024-06-04T16:31:13.043529Z","shell.execute_reply.started":"2024-06-04T16:31:13.034860Z","shell.execute_reply":"2024-06-04T16:31:13.042597Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"classID\n0    1885\n1    1536\n2    1272\n3     704\n4     634\n5     419\nName: count, dtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"# Construct file path by concatenating fold and file name\ndf['relative_path'] = '/' + df['Filename'].astype(str)\n\n# Take relevant columns\ndf = df[['relative_path', 'classID']]\ndf.head()","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"cc1913ad","outputId":"829d7a2a-133d-4734-d817-b1795e3c586d","execution":{"iopub.status.busy":"2024-06-04T16:31:13.044779Z","iopub.execute_input":"2024-06-04T16:31:13.045102Z","iopub.status.idle":"2024-06-04T16:31:13.060729Z","shell.execute_reply.started":"2024-06-04T16:31:13.045073Z","shell.execute_reply":"2024-06-04T16:31:13.059893Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"                                relative_path  classID\n0   /210408_2025_00-01-38.27--00-01-40.84.wav        4\n1   /210324_2036_00-06-03.61--00-06-05.78.wav        4\n2   /210324_2036_00-09-04.66--00-09-06.16.wav        4\n3     /210324_2036_00-11-18.6--00-11-20.3.wav        4\n4  /200506_2110_00-01-25.92--00-01-26.58c.wav        4","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>relative_path</th>\n      <th>classID</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>/210408_2025_00-01-38.27--00-01-40.84.wav</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>/210324_2036_00-06-03.61--00-06-05.78.wav</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>/210324_2036_00-09-04.66--00-09-06.16.wav</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>/210324_2036_00-11-18.6--00-11-20.3.wav</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>/200506_2110_00-01-25.92--00-01-26.58c.wav</td>\n      <td>4</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"#Audio Pre-processing","metadata":{"id":"704c430c"}},{"cell_type":"markdown","source":"Audio data is memory intensive. So, we don’t want to read the entire dataset into memory all at once. Instead, we keep only the audio file names in our training data and perform pre-processing dynamically during runtime.","metadata":{"id":"e0e30cc0"}},{"cell_type":"markdown","source":"Then, at runtime, as we train the model one batch at a time, we will load the audio data for that batch and process it by applying a series of transforms to the audio. That way we keep audio data for only one batch in memory at a time.","metadata":{"id":"61f0be91"}},{"cell_type":"code","source":"#Random shuffle of rows.\ndf = df. sample(frac=1)\ndf.head()","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"3d6bb31a","outputId":"5d9ff441-ee9c-4340-9d2d-aa8aa4da890e","execution":{"iopub.status.busy":"2024-06-04T16:31:13.061790Z","iopub.execute_input":"2024-06-04T16:31:13.062047Z","iopub.status.idle":"2024-06-04T16:31:13.075391Z","shell.execute_reply.started":"2024-06-04T16:31:13.062026Z","shell.execute_reply":"2024-06-04T16:31:13.074478Z"},"trusted":true},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"                                  relative_path  classID\n5089  /200121_2034_00-01-48.38--00-01-49.02.wav        1\n2660  /200411_1548_00-42-58.86--00-43-00.35.wav        3\n366   /200329_1113_00-05-30.03--00-05-30.99.wav        0\n4889  /210205_1724_00-03-38.45--00-03-39.35.wav        5\n2859  /200219_0835_00-06-03.61--00-06-04.06.wav        0","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>relative_path</th>\n      <th>classID</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>5089</th>\n      <td>/200121_2034_00-01-48.38--00-01-49.02.wav</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2660</th>\n      <td>/200411_1548_00-42-58.86--00-43-00.35.wav</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>366</th>\n      <td>/200329_1113_00-05-30.03--00-05-30.99.wav</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4889</th>\n      <td>/210205_1724_00-03-38.45--00-03-39.35.wav</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>2859</th>\n      <td>/200219_0835_00-06-03.61--00-06-04.06.wav</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"#Function to read and load the audio files in '.wav' format using Pytorch.\nclass AudioUtil():\n  @staticmethod\n  def open(audio_file):\n    sig, sr = torchaudio.load(audio_file) # Load an audio file. Return the signal as a tensor and the sample rate\n    return (sig, sr)\n\n\n#Refer tutotial file for explaination about the following blocks of code: -\n  def rechannel(aud, new_channel):\n    sig, sr = aud\n    if (sig.shape[0] == new_channel):\n      return aud\n    if (new_channel == 1):\n      resig = sig[:1, :]\n    else:\n      resig = torch.cat([sig, sig])\n    return ((resig, sr))\n\n  def resample(aud, newsr):\n    sig, sr = aud\n    if (sr == newsr):\n      return aud\n    num_channels = sig.shape[0]\n    resig = torchaudio.transforms.Resample(sr, newsr)(sig[:1,:])\n    if (num_channels > 1):\n      retwo = torchaudio.transforms.Resample(sr, newsr)(sig[1:,:])\n      resig = torch.cat([resig, retwo])\n    return ((resig, newsr))\n\n  def pad_trunc(aud, max_ms):\n    sig, sr = aud\n    num_rows, sig_len = sig.shape\n    max_len = sr//1000 * max_ms\n    if (sig_len > max_len):\n      sig = sig[:,:max_len]\n    elif (sig_len < max_len):\n      pad_begin_len = random.randint(0, max_len - sig_len)\n      pad_end_len = max_len - sig_len - pad_begin_len\n      pad_begin = torch.zeros((num_rows, pad_begin_len))\n      pad_end = torch.zeros((num_rows, pad_end_len))\n      sig = torch.cat((pad_begin, sig, pad_end), 1)\n    return (sig, sr)\n\n  def time_shift(aud, shift_limit):\n    sig,sr = aud\n    _, sig_len = sig.shape\n    shift_amt = int(random.random() * shift_limit * sig_len)\n    return (sig.roll(shift_amt), sr)\n\n  def spectro_gram(aud, n_mels=64, n_fft=1024, hop_len=None):\n    sig,sr = aud\n    top_db = 80\n    spec = transforms.MelSpectrogram(sr, n_fft=n_fft, hop_length=hop_len, n_mels=n_mels)(sig)\n    spec = transforms.AmplitudeToDB(top_db=top_db)(spec)\n    return (spec)\n\n  def spectro_augment(spec, max_mask_pct=0.1, n_freq_masks=1, n_time_masks=1):\n    _, n_mels, n_steps = spec.shape\n    mask_value = spec.mean()\n    aug_spec = spec\n    freq_mask_param = max_mask_pct * n_mels\n    for _ in range(n_freq_masks):\n      aug_spec = transforms.FrequencyMasking(freq_mask_param)(aug_spec, mask_value)\n    time_mask_param = max_mask_pct * n_steps\n    for _ in range(n_time_masks):\n      aug_spec = transforms.TimeMasking(time_mask_param)(aug_spec, mask_value)\n    return aug_spec","metadata":{"id":"902a8202","execution":{"iopub.status.busy":"2024-06-04T16:31:13.076731Z","iopub.execute_input":"2024-06-04T16:31:13.077046Z","iopub.status.idle":"2024-06-04T16:31:13.092458Z","shell.execute_reply.started":"2024-06-04T16:31:13.077021Z","shell.execute_reply":"2024-06-04T16:31:13.091547Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"#Define Custom Data Loader\n","metadata":{"id":"d9ab827d"}},{"cell_type":"markdown","source":"To feed the data to a model with Pytorch, we need two objects:\n\n*   A custom Dataset object that uses all the audio transforms to pre-process an audio file and prepare one data item at a time.\n*   A built-in DataLoader object that uses the Dataset object to fetch individual data items and packages them into a batch of data.","metadata":{"id":"d3812ea4"}},{"cell_type":"code","source":"class SoundDS(Dataset):\n  def __init__(self, df, data_path):\n    self.df = df\n    self.data_path = str(data_path)\n    self.duration = 4000\n    self.sr = 44100\n    self.channel = 2\n    self.shift_pct = 0.4\n\n  #Number of items in dataset\n  def __len__(self):\n    return len(self.df)\n\n\n  #Get i'th item in dataset\n  def __getitem__(self, idx):\n    #Absolute file path of the audio file = audio directory + relative path\n    audio_file = self.data_path + self.df.loc[idx, 'relative_path']\n    #Get the Class ID\n    class_id = self.df.loc[idx, 'classID']\n\n    aud = AudioUtil.open(audio_file)\n    \"\"\"\n    Some sounds have a higher sample rate, or fewer channels compared to the\n    majority. So make all sounds have the same number of channels and same\n    sample rate. Unless the sample rate is the same, the pad_trunc will still\n    result in arrays of different lengths, even though the sound duration is\n    the same.\n    \"\"\"\n    reaud = AudioUtil.resample(aud, self.sr)\n    rechan = AudioUtil.rechannel(reaud, self.channel)\n\n\n    dur_aud = AudioUtil.pad_trunc(rechan, self.duration)\n    shift_aud = AudioUtil.time_shift(dur_aud, self.shift_pct)\n    sgram = AudioUtil.spectro_gram(shift_aud, n_mels=64, n_fft=1024, hop_len=None)\n    aug_sgram = AudioUtil.spectro_augment(sgram, max_mask_pct=0.1, n_freq_masks=2, n_time_masks=2)\n\n    return aug_sgram, class_id","metadata":{"id":"e28bcd41","execution":{"iopub.status.busy":"2024-06-04T16:31:13.093554Z","iopub.execute_input":"2024-06-04T16:31:13.093833Z","iopub.status.idle":"2024-06-04T16:31:13.105519Z","shell.execute_reply.started":"2024-06-04T16:31:13.093811Z","shell.execute_reply":"2024-06-04T16:31:13.104635Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"#Prepare Batches of Data with the Data Loader","metadata":{"id":"37d39e00"}},{"cell_type":"markdown","source":"Load the Features and Labels from our Pandas dataframe and split it in an 80:20 ratio for training and validation. Then, use them to create the training and validation Data Loaders.","metadata":{"id":"5e70e2d5"}},{"cell_type":"code","source":"data_path = \"/kaggle/input/autism-paper-data/Retained/audio\"\nmyds = SoundDS(df, data_path)\n#myds = SoundDS(df, path)\n\n#Random split of 80:20 between training and validation\nnum_items = len(myds)\nnum_train = round(num_items * 0.8)\nnum_val = num_items - num_train\ntrain_ds, val_ds = random_split(myds, [num_train, num_val])\n\n#Create training and validation data loaders\ntrain_dl = torch.utils.data.DataLoader(train_ds, batch_size=16, shuffle=True)\nval_dl = torch.utils.data.DataLoader(val_ds, batch_size=16, shuffle=False)","metadata":{"id":"64177b8f","execution":{"iopub.status.busy":"2024-06-04T16:31:13.106629Z","iopub.execute_input":"2024-06-04T16:31:13.106947Z","iopub.status.idle":"2024-06-04T16:31:13.129270Z","shell.execute_reply.started":"2024-06-04T16:31:13.106923Z","shell.execute_reply":"2024-06-04T16:31:13.128376Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"print(num_items, num_train, num_val)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7bd25150","outputId":"d6280b4e-080b-4a18-8921-b2e43c222ca7","execution":{"iopub.status.busy":"2024-06-04T16:31:13.130249Z","iopub.execute_input":"2024-06-04T16:31:13.130512Z","iopub.status.idle":"2024-06-04T16:31:13.135224Z","shell.execute_reply.started":"2024-06-04T16:31:13.130490Z","shell.execute_reply":"2024-06-04T16:31:13.134356Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"6450 5160 1290\n","output_type":"stream"}]},{"cell_type":"code","source":"type(train_dl)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"c32454d9","outputId":"599600ce-082b-4a90-ab07-aea5a8c1ab12","execution":{"iopub.status.busy":"2024-06-04T16:31:13.136498Z","iopub.execute_input":"2024-06-04T16:31:13.136920Z","iopub.status.idle":"2024-06-04T16:31:13.145161Z","shell.execute_reply.started":"2024-06-04T16:31:13.136888Z","shell.execute_reply":"2024-06-04T16:31:13.144297Z"},"trusted":true},"execution_count":19,"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"torch.utils.data.dataloader.DataLoader"},"metadata":{}}]},{"cell_type":"markdown","source":"During training, the Data Loader will randomly fetch one batch of input features containing the list of audio file names and run the pre-processing audio transforms on each audio file. It will also fetch the corresponding target labels containing the class IDs. Thus it will output one batch of training data at a time, which can directly be fed as input to our DL model.","metadata":{"id":"701a3222"}},{"cell_type":"markdown","source":"\n\n*   The audio from the file gets loaded into a Numpy array of shape (num_channels, num_samples). Most of the audio is sampled at 44.1kHz and is about 4 seconds in duration, resulting in 44,100 * 4 = 176,400 samples. If the audio has 1 channel, the shape of the array will be (1, 176400). Similarly, audio of 4 seconds duration with 2 channels and sampled at 48kHz will have 192,000 samples and a shape of (2, 192000).\n*   Since the channels and sampling rates of each audio are different, the next two transforms resample the audio to a standard 44.1kHz and to a standard 2 channels.\n*   Since some audio clips might be more or less than 4 seconds, we also standardize the audio duration to a fixed length of 4 seconds. Now arrays for all items have the same shape of (2, 176,400).\n*   The Time Shift data augmentation now randomly shifts each audio sample forward or backward. The shapes are unchanged.\n*   The augmented audio is now converted into a Mel Spectrogram, resulting in a shape of (num_channels, Mel freq_bands, time_steps) = (2, 64, 344).\n*   The SpecAugment data augmentation now randomly applies Time and Frequency Masks to the Mel Spectrograms. The shapes are unchanged.\n\nThus, each batch will have two tensors, one for the X feature data containing the Mel Spectrograms and the other for the y target labels containing numeric Class IDs. The batches are picked randomly from the training data for each training epoch.\nEach batch has a shape of (batch_sz, num_channels, Mel freq_bands, time_steps).\n\n\n\n\n","metadata":{"id":"22183585"}},{"cell_type":"code","source":"\"\"\"\n#From now on, our data consists of only images. So we can deal with it using CNNs as usual.\n#We build a model with 4 convolutional blocks which generate the feature maps.\n#That data is then reshaped into the reqd format so that it can be input into the linear classifier layer, which makes the final predictions.\n\n\n#A batch of images is input to the model with shape (batch_sz, num_channels, Mel freq_bands, time_steps) i.e., (16, 2, 64, 344).\n#Each CNN layer applies its filters to step up the image depth i.e., number of channels.\n#The image width and height are reduced as the kernels and strides are applied.\n#Finally, after passing through the four CNN layers, we get the output feature maps i.e., (16, 64, 4, 22).\n#This gets pooled and flattened to a shape of (16, 64) and then input to the Linear layer.\n#The Linear layer outputs one prediction score per class ie. (16, 10).\n\n# ----------------------------\n# Audio Classification Model\n# ----------------------------\nclass AudioClassifier (nn.Module):\n    # ----------------------------\n    # Build the model architecture\n    # ----------------------------\n    def __init__(self):\n        super().__init__()\n        conv_layers = []\n\n        # First Convolution Block with Relu and Batch Norm. Use Kaiming Initialization\n        self.conv1 = nn.Conv2d(2, 16, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n        self.relu1 = nn.ReLU()\n        self.bn1 = nn.BatchNorm2d(16)\n        init.kaiming_normal_(self.conv1.weight, a=0.1)\n        self.conv1.bias.data.zero_()\n        conv_layers += [self.conv1, self.relu1, self.bn1]\n\n        # Second Convolution Block\n        self.conv2 = nn.Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n        self.relu2 = nn.ReLU()\n        self.bn2 = nn.BatchNorm2d(32)\n        init.kaiming_normal_(self.conv2.weight, a=0.1)\n        self.conv2.bias.data.zero_()\n        conv_layers += [self.conv2, self.relu2, self.bn2]\n\n        # Third Convolution Block\n        self.conv3 = nn.Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n        self.relu3 = nn.ReLU()\n        self.bn3 = nn.BatchNorm2d(64)\n        init.kaiming_normal_(self.conv3.weight, a=0.1)\n        self.conv3.bias.data.zero_()\n        conv_layers += [self.conv3, self.relu3, self.bn3]\n\n        # Fourth Convolution Block\n        self.conv4 = nn.Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n        self.relu4 = nn.ReLU()\n        self.bn4 = nn.BatchNorm2d(128)\n        init.kaiming_normal_(self.conv4.weight, a=0.1)\n        self.conv4.bias.data.zero_()\n        conv_layers += [self.conv4, self.relu4, self.bn4]\n\n        # Fifth Convolution Block\n        self.conv4 = nn.Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n        self.relu4 = nn.ReLU()\n        self.bn4 = nn.BatchNorm2d(256)\n        init.kaiming_normal_(self.conv4.weight, a=0.1)\n        self.conv4.bias.data.zero_()\n        conv_layers += [self.conv4, self.relu4, self.bn4]\n\n        # Sixth Convolution Block\n        self.conv4 = nn.Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n        self.relu4 = nn.ReLU()\n        self.bn4 = nn.BatchNorm2d(512)\n        init.kaiming_normal_(self.conv4.weight, a=0.1)\n        self.conv4.bias.data.zero_()\n        conv_layers += [self.conv4, self.relu4, self.bn4]\n\n        # Seventh Convolution Block\n        self.conv4 = nn.Conv2d(512, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n        self.relu4 = nn.ReLU()\n        self.bn4 = nn.BatchNorm2d(1024)\n        init.kaiming_normal_(self.conv4.weight, a=0.1)\n        self.conv4.bias.data.zero_()\n        conv_layers += [self.conv4, self.relu4, self.bn4]\n\n        # Eight Convolution Block\n        self.conv4 = nn.Conv2d(1024, 2048, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n        self.relu4 = nn.ReLU()\n        self.bn4 = nn.BatchNorm2d(2048)\n        init.kaiming_normal_(self.conv4.weight, a=0.1)\n        self.conv4.bias.data.zero_()\n        conv_layers += [self.conv4, self.relu4, self.bn4]\n\n        # Linear Classifier\n        self.ap = nn.AdaptiveAvgPool2d(output_size=1)\n        #self.lin = nn.Linear(in_features=64, out_features=22)\n        self.lin = nn.Linear(in_features=2048, out_features=6)\n\n        # Wrap the Convolutional Blocks\n        self.conv = nn.Sequential(*conv_layers)\n\n    # ----------------------------\n    # Forward pass computations\n    # ----------------------------\n    def forward(self, x):\n        # Run the convolutional blocks\n        x = self.conv(x)\n\n        # Adaptive pool and flatten for input to linear layer\n        x = self.ap(x)\n        x = x.view(x.shape[0], -1)\n\n        # Linear layer\n        x = self.lin(x)\n\n        # Final output\n        return x\n\n# Create the model and put it on the GPU if available\nmyModel = AudioClassifier()\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nmyModel = myModel.to(device)\n# Check that it is on Cuda\nnext(myModel.parameters()).device\n\"\"\"","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":232},"id":"06ea9bd0","outputId":"af20c274-040e-4a06-a611-d9df67cb7442","execution":{"iopub.status.busy":"2024-06-04T16:31:13.146338Z","iopub.execute_input":"2024-06-04T16:31:13.146600Z","iopub.status.idle":"2024-06-04T16:31:13.160493Z","shell.execute_reply.started":"2024-06-04T16:31:13.146575Z","shell.execute_reply":"2024-06-04T16:31:13.159598Z"},"trusted":true},"execution_count":20,"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"'\\n#From now on, our data consists of only images. So we can deal with it using CNNs as usual.\\n#We build a model with 4 convolutional blocks which generate the feature maps.\\n#That data is then reshaped into the reqd format so that it can be input into the linear classifier layer, which makes the final predictions.\\n\\n\\n#A batch of images is input to the model with shape (batch_sz, num_channels, Mel freq_bands, time_steps) i.e., (16, 2, 64, 344).\\n#Each CNN layer applies its filters to step up the image depth i.e., number of channels.\\n#The image width and height are reduced as the kernels and strides are applied.\\n#Finally, after passing through the four CNN layers, we get the output feature maps i.e., (16, 64, 4, 22).\\n#This gets pooled and flattened to a shape of (16, 64) and then input to the Linear layer.\\n#The Linear layer outputs one prediction score per class ie. (16, 10).\\n\\n# ----------------------------\\n# Audio Classification Model\\n# ----------------------------\\nclass AudioClassifier (nn.Module):\\n    # ----------------------------\\n    # Build the model architecture\\n    # ----------------------------\\n    def __init__(self):\\n        super().__init__()\\n        conv_layers = []\\n\\n        # First Convolution Block with Relu and Batch Norm. Use Kaiming Initialization\\n        self.conv1 = nn.Conv2d(2, 16, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\\n        self.relu1 = nn.ReLU()\\n        self.bn1 = nn.BatchNorm2d(16)\\n        init.kaiming_normal_(self.conv1.weight, a=0.1)\\n        self.conv1.bias.data.zero_()\\n        conv_layers += [self.conv1, self.relu1, self.bn1]\\n\\n        # Second Convolution Block\\n        self.conv2 = nn.Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\\n        self.relu2 = nn.ReLU()\\n        self.bn2 = nn.BatchNorm2d(32)\\n        init.kaiming_normal_(self.conv2.weight, a=0.1)\\n        self.conv2.bias.data.zero_()\\n        conv_layers += [self.conv2, self.relu2, self.bn2]\\n\\n        # Third Convolution Block\\n        self.conv3 = nn.Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\\n        self.relu3 = nn.ReLU()\\n        self.bn3 = nn.BatchNorm2d(64)\\n        init.kaiming_normal_(self.conv3.weight, a=0.1)\\n        self.conv3.bias.data.zero_()\\n        conv_layers += [self.conv3, self.relu3, self.bn3]\\n\\n        # Fourth Convolution Block\\n        self.conv4 = nn.Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\\n        self.relu4 = nn.ReLU()\\n        self.bn4 = nn.BatchNorm2d(128)\\n        init.kaiming_normal_(self.conv4.weight, a=0.1)\\n        self.conv4.bias.data.zero_()\\n        conv_layers += [self.conv4, self.relu4, self.bn4]\\n\\n        # Fifth Convolution Block\\n        self.conv4 = nn.Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\\n        self.relu4 = nn.ReLU()\\n        self.bn4 = nn.BatchNorm2d(256)\\n        init.kaiming_normal_(self.conv4.weight, a=0.1)\\n        self.conv4.bias.data.zero_()\\n        conv_layers += [self.conv4, self.relu4, self.bn4]\\n\\n        # Sixth Convolution Block\\n        self.conv4 = nn.Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\\n        self.relu4 = nn.ReLU()\\n        self.bn4 = nn.BatchNorm2d(512)\\n        init.kaiming_normal_(self.conv4.weight, a=0.1)\\n        self.conv4.bias.data.zero_()\\n        conv_layers += [self.conv4, self.relu4, self.bn4]\\n\\n        # Seventh Convolution Block\\n        self.conv4 = nn.Conv2d(512, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\\n        self.relu4 = nn.ReLU()\\n        self.bn4 = nn.BatchNorm2d(1024)\\n        init.kaiming_normal_(self.conv4.weight, a=0.1)\\n        self.conv4.bias.data.zero_()\\n        conv_layers += [self.conv4, self.relu4, self.bn4]\\n\\n        # Eight Convolution Block\\n        self.conv4 = nn.Conv2d(1024, 2048, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\\n        self.relu4 = nn.ReLU()\\n        self.bn4 = nn.BatchNorm2d(2048)\\n        init.kaiming_normal_(self.conv4.weight, a=0.1)\\n        self.conv4.bias.data.zero_()\\n        conv_layers += [self.conv4, self.relu4, self.bn4]\\n\\n        # Linear Classifier\\n        self.ap = nn.AdaptiveAvgPool2d(output_size=1)\\n        #self.lin = nn.Linear(in_features=64, out_features=22)\\n        self.lin = nn.Linear(in_features=2048, out_features=6)\\n\\n        # Wrap the Convolutional Blocks\\n        self.conv = nn.Sequential(*conv_layers)\\n\\n    # ----------------------------\\n    # Forward pass computations\\n    # ----------------------------\\n    def forward(self, x):\\n        # Run the convolutional blocks\\n        x = self.conv(x)\\n\\n        # Adaptive pool and flatten for input to linear layer\\n        x = self.ap(x)\\n        x = x.view(x.shape[0], -1)\\n\\n        # Linear layer\\n        x = self.lin(x)\\n\\n        # Final output\\n        return x\\n\\n# Create the model and put it on the GPU if available\\nmyModel = AudioClassifier()\\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\\nmyModel = myModel.to(device)\\n# Check that it is on Cuda\\nnext(myModel.parameters()).device\\n'"},"metadata":{}}]},{"cell_type":"code","source":"import torchvision.models as models\n#  Drop in whichever pretrained model you like from https://pytorch.org/vision/0.8/models.html\nmodel = models.vgg19_bn(pretrained=True)\nmodel.features[0] = nn.Conv2d(2, model.features[0].out_channels,\n                               kernel_size=model.features[0].kernel_size,\n                               stride=model.features[0].stride,\n                               padding=model.features[0].padding)\nnum_ftrs = model.classifier[-1].in_features\nmodel.classifier[-1] = nn.Linear(num_ftrs, df_original['Label'].nunique())\n\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nmyModel = model.to(device)\n\n# Check that it is on Cuda\nnext(myModel.parameters()).device","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8f0febab","outputId":"1b67ee77-e334-4a20-8c2b-de2426ddbe2a","execution":{"iopub.status.busy":"2024-06-04T16:31:13.161546Z","iopub.execute_input":"2024-06-04T16:31:13.161806Z","iopub.status.idle":"2024-06-04T16:31:18.762502Z","shell.execute_reply.started":"2024-06-04T16:31:13.161783Z","shell.execute_reply":"2024-06-04T16:31:18.761586Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG19_BN_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_BN_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/vgg19_bn-c79401a0.pth\" to /root/.cache/torch/hub/checkpoints/vgg19_bn-c79401a0.pth\n100%|██████████| 548M/548M [00:03<00:00, 169MB/s]  \n","output_type":"stream"},{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"device(type='cuda', index=0)"},"metadata":{}}]},{"cell_type":"code","source":"#Train the model for multiple epochs by processing one batch of data in each iteration. Use accuracy as the metric.\ndef training(model, train_dl, num_epochs):\n  # Loss Function, Optimizer and Scheduler\n  criterion = nn.CrossEntropyLoss()\n  optimizer = torch.optim.Adam(model.parameters(),lr=0.0001)\n  scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.0001,\n                                                steps_per_epoch=int(len(train_dl)),\n                                                epochs=num_epochs,\n                                                anneal_strategy='linear')\n\n  # Repeat for each epoch\n  for epoch in range(num_epochs):\n    running_loss = 0.0\n    correct_prediction = 0\n    total_prediction = 0\n\n    # Repeat for each batch in the training set\n    for i, data in enumerate(train_dl):\n        # Get the input features and target labels, and put them on the GPU\n        inputs, labels = data[0].to(device), data[1].to(device)\n\n        # Normalize the inputs\n        inputs_m, inputs_s = inputs.mean(), inputs.std()\n        inputs = (inputs - inputs_m) / inputs_s\n\n        # Zero the parameter gradients\n        optimizer.zero_grad()\n\n        # forward + backward + optimize\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        scheduler.step()\n\n        # Keep stats for Loss and Accuracy\n        running_loss += loss.item()\n\n        # Get the predicted class with the highest score\n        _, prediction = torch.max(outputs,1)\n        # Count of predictions that matched the target label\n        correct_prediction += (prediction == labels).sum().item()\n        total_prediction += prediction.shape[0]\n\n        #if i % 10 == 0:    # print every 10 mini-batches\n        #    print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss / 10))\n\n    # Print stats at the end of the epoch\n    num_batches = len(train_dl)\n    avg_loss = running_loss / num_batches\n    acc = correct_prediction/total_prediction\n    print(f'Epoch: {epoch}, Loss: {avg_loss:.2f}, Accuracy: {acc:.2f}')\n\n  print('Finished Training')\n\n\n\nnum_epochs = 50\ntraining(myModel, train_dl, num_epochs)","metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"32dd2760","outputId":"4d41cfc3-8082-4ba5-e343-3834b92ea640","execution":{"iopub.status.busy":"2024-06-04T16:31:18.763775Z","iopub.execute_input":"2024-06-04T16:31:18.764053Z","iopub.status.idle":"2024-06-04T17:58:09.779989Z","shell.execute_reply.started":"2024-06-04T16:31:18.764029Z","shell.execute_reply":"2024-06-04T17:58:09.779030Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"Epoch: 0, Loss: 1.67, Accuracy: 0.30\nEpoch: 1, Loss: 1.51, Accuracy: 0.40\nEpoch: 2, Loss: 1.38, Accuracy: 0.46\nEpoch: 3, Loss: 1.28, Accuracy: 0.51\nEpoch: 4, Loss: 1.20, Accuracy: 0.54\nEpoch: 5, Loss: 1.16, Accuracy: 0.57\nEpoch: 6, Loss: 1.11, Accuracy: 0.59\nEpoch: 7, Loss: 1.06, Accuracy: 0.61\nEpoch: 8, Loss: 1.02, Accuracy: 0.62\nEpoch: 9, Loss: 1.00, Accuracy: 0.64\nEpoch: 10, Loss: 0.99, Accuracy: 0.64\nEpoch: 11, Loss: 0.94, Accuracy: 0.66\nEpoch: 12, Loss: 0.95, Accuracy: 0.66\nEpoch: 13, Loss: 0.92, Accuracy: 0.67\nEpoch: 14, Loss: 0.89, Accuracy: 0.67\nEpoch: 15, Loss: 0.86, Accuracy: 0.69\nEpoch: 16, Loss: 0.81, Accuracy: 0.71\nEpoch: 17, Loss: 0.78, Accuracy: 0.72\nEpoch: 18, Loss: 0.74, Accuracy: 0.73\nEpoch: 19, Loss: 0.72, Accuracy: 0.73\nEpoch: 20, Loss: 0.67, Accuracy: 0.76\nEpoch: 21, Loss: 0.62, Accuracy: 0.78\nEpoch: 22, Loss: 0.59, Accuracy: 0.78\nEpoch: 23, Loss: 0.55, Accuracy: 0.80\nEpoch: 24, Loss: 0.52, Accuracy: 0.82\nEpoch: 25, Loss: 0.48, Accuracy: 0.83\nEpoch: 26, Loss: 0.45, Accuracy: 0.84\nEpoch: 27, Loss: 0.42, Accuracy: 0.85\nEpoch: 28, Loss: 0.36, Accuracy: 0.87\nEpoch: 29, Loss: 0.35, Accuracy: 0.87\nEpoch: 30, Loss: 0.30, Accuracy: 0.89\nEpoch: 31, Loss: 0.27, Accuracy: 0.90\nEpoch: 32, Loss: 0.26, Accuracy: 0.91\nEpoch: 33, Loss: 0.25, Accuracy: 0.91\nEpoch: 34, Loss: 0.24, Accuracy: 0.91\nEpoch: 35, Loss: 0.21, Accuracy: 0.92\nEpoch: 36, Loss: 0.19, Accuracy: 0.93\nEpoch: 37, Loss: 0.17, Accuracy: 0.94\nEpoch: 38, Loss: 0.13, Accuracy: 0.95\nEpoch: 39, Loss: 0.15, Accuracy: 0.94\nEpoch: 40, Loss: 0.14, Accuracy: 0.95\nEpoch: 41, Loss: 0.12, Accuracy: 0.96\nEpoch: 42, Loss: 0.12, Accuracy: 0.96\nEpoch: 43, Loss: 0.10, Accuracy: 0.96\nEpoch: 44, Loss: 0.10, Accuracy: 0.96\nEpoch: 45, Loss: 0.09, Accuracy: 0.97\nEpoch: 46, Loss: 0.08, Accuracy: 0.97\nEpoch: 47, Loss: 0.07, Accuracy: 0.97\nEpoch: 48, Loss: 0.07, Accuracy: 0.98\nEpoch: 49, Loss: 0.07, Accuracy: 0.98\nFinished Training\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#Inference","metadata":{"id":"da08964c"}},{"cell_type":"code","source":"#Testing and validation (we use validation data here, instead of external data sets).\n#We run an inference loop taking care to disable the gradient updates.\n#The forward pass is executed with the model to get predictions, but we do not need to backpropagate or run the optimizer.\n\ndef inference (model, val_dl):\n  correct_prediction = 0\n  total_prediction = 0\n\n  # Disable gradient updates\n  with torch.no_grad():\n    for data in val_dl:\n      # Get the input features and target labels, and put them on the GPU\n      inputs, labels = data[0].to(device), data[1].to(device)\n\n      # Normalize the inputs\n      inputs_m, inputs_s = inputs.mean(), inputs.std()\n      inputs = (inputs - inputs_m) / inputs_s\n\n      # Get predictions\n      outputs = model(inputs)\n\n      # Get the predicted class with the highest score\n      _, prediction = torch.max(outputs,1)\n      # Count of predictions that matched the target label\n      correct_prediction += (prediction == labels).sum().item()\n      total_prediction += prediction.shape[0]\n\n  acc = correct_prediction/total_prediction\n  print(f'Accuracy: {acc:.2f}, Total items: {total_prediction}')\n\n# Run inference on trained model with the validation set\ninference(myModel, val_dl)","metadata":{"id":"81cc43e0","execution":{"iopub.status.busy":"2024-06-04T18:57:36.128659Z","iopub.execute_input":"2024-06-04T18:57:36.129043Z","iopub.status.idle":"2024-06-04T18:58:02.228172Z","shell.execute_reply.started":"2024-06-04T18:57:36.129013Z","shell.execute_reply":"2024-06-04T18:58:02.227122Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"Accuracy: 0.72, Total items: 1290\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.metrics import f1_score, roc_auc_score\nimport numpy as np\n\ndef inference(model, val_dl):\n    all_predictions = []\n    all_labels = []\n    \n    # Disable gradient updates\n    with torch.no_grad():\n        for data in val_dl:\n            inputs, labels = data[0].to(device), data[1].to(device)\n\n            # Normalize the inputs\n            inputs_m, inputs_s = inputs.mean(), inputs.std()\n            inputs = (inputs - inputs_m) / inputs_s\n\n            # Get predictions\n            outputs = model(inputs)\n            _, predictions = torch.max(outputs, 1)\n            \n            all_predictions.extend(predictions.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n\n    # Calculate metrics\n    accuracy = np.mean(np.array(all_predictions) == np.array(all_labels))\n    f1 = f1_score(all_labels, all_predictions, average='weighted')\n        \n    print(f'Accuracy: {accuracy:.2f}, F1 Score: {f1:.2f}')\n\n# Run inference on trained model with the validation set\ninference(myModel, val_dl)","metadata":{"execution":{"iopub.status.busy":"2024-06-04T19:00:11.429968Z","iopub.execute_input":"2024-06-04T19:00:11.430365Z","iopub.status.idle":"2024-06-04T19:00:31.993114Z","shell.execute_reply.started":"2024-06-04T19:00:11.430335Z","shell.execute_reply":"2024-06-04T19:00:31.992033Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"Accuracy: 0.72, F1 Score: 0.72\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}