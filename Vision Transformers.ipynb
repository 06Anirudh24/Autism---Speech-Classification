{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8588058,"sourceType":"datasetVersion","datasetId":5136800}],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras import datasets, layers, models\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport math, random\nimport torch\nimport torchaudio\nfrom torchaudio import transforms\nfrom IPython.display import Audio\nimport timm\nfrom torch.utils.data import DataLoader, Dataset, random_split\nimport torch.nn.functional as F\nfrom torch.nn import init\nimport torch\nimport torchvision\nimport torch.nn as nn\nfrom torchvision import transforms\nfrom torchvision import models","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-06-21T16:33:32.494053Z","iopub.execute_input":"2024-06-21T16:33:32.494458Z","iopub.status.idle":"2024-06-21T16:33:32.502508Z","shell.execute_reply.started":"2024-06-21T16:33:32.494426Z","shell.execute_reply":"2024-06-21T16:33:32.501273Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"path = \"/kaggle/input/autism-paper-data/Retained/dataset_file_directory.csv\"\ndf_original = pd.read_csv(path)\ndf_original.head()","metadata":{"execution":{"iopub.status.busy":"2024-06-21T16:33:32.523423Z","iopub.execute_input":"2024-06-21T16:33:32.523815Z","iopub.status.idle":"2024-06-21T16:33:32.566033Z","shell.execute_reply.started":"2024-06-21T16:33:32.523783Z","shell.execute_reply":"2024-06-21T16:33:32.564891Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"                                    Filename Participant   Label\n0   210408_2025_00-01-38.27--00-01-40.84.wav         P01  social\n1   210324_2036_00-06-03.61--00-06-05.78.wav         P01  social\n2   210324_2036_00-09-04.66--00-09-06.16.wav         P01  social\n3     210324_2036_00-11-18.6--00-11-20.3.wav         P01  social\n4  200506_2110_00-01-25.92--00-01-26.58c.wav         P01  social","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Filename</th>\n      <th>Participant</th>\n      <th>Label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>210408_2025_00-01-38.27--00-01-40.84.wav</td>\n      <td>P01</td>\n      <td>social</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>210324_2036_00-06-03.61--00-06-05.78.wav</td>\n      <td>P01</td>\n      <td>social</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>210324_2036_00-09-04.66--00-09-06.16.wav</td>\n      <td>P01</td>\n      <td>social</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>210324_2036_00-11-18.6--00-11-20.3.wav</td>\n      <td>P01</td>\n      <td>social</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>200506_2110_00-01-25.92--00-01-26.58c.wav</td>\n      <td>P01</td>\n      <td>social</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"df_original.shape[0]","metadata":{"execution":{"iopub.status.busy":"2024-06-21T16:33:32.567966Z","iopub.execute_input":"2024-06-21T16:33:32.568388Z","iopub.status.idle":"2024-06-21T16:33:32.575344Z","shell.execute_reply.started":"2024-06-21T16:33:32.568356Z","shell.execute_reply":"2024-06-21T16:33:32.574264Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"6450"},"metadata":{}}]},{"cell_type":"code","source":"#df_original.Label.unique() #Print unique types of sound","metadata":{"execution":{"iopub.status.busy":"2024-06-21T16:33:32.576572Z","iopub.execute_input":"2024-06-21T16:33:32.576945Z","iopub.status.idle":"2024-06-21T16:33:32.583820Z","shell.execute_reply.started":"2024-06-21T16:33:32.576912Z","shell.execute_reply":"2024-06-21T16:33:32.582672Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"df_original['Label'].value_counts() #Number of datapoints available for each target variable","metadata":{"execution":{"iopub.status.busy":"2024-06-21T16:33:32.587018Z","iopub.execute_input":"2024-06-21T16:33:32.587526Z","iopub.status.idle":"2024-06-21T16:33:32.604315Z","shell.execute_reply.started":"2024-06-21T16:33:32.587496Z","shell.execute_reply":"2024-06-21T16:33:32.603350Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"Label\nselftalk        1885\nfrustrated      1536\ndelighted       1272\ndysregulated     704\nsocial           634\nrequest          419\nName: count, dtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"df_original[\"classID\"] = df_original['Label']\ndf_original['classID'].replace(['selftalk', 'frustrated', 'delighted', 'dysregulated', 'social', 'request'],\n                        [0, 1, 2, 3, 4, 5], inplace=True)","metadata":{"execution":{"iopub.status.busy":"2024-06-21T16:33:32.605862Z","iopub.execute_input":"2024-06-21T16:33:32.606180Z","iopub.status.idle":"2024-06-21T16:33:32.622094Z","shell.execute_reply.started":"2024-06-21T16:33:32.606152Z","shell.execute_reply":"2024-06-21T16:33:32.620946Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_34/808862154.py:2: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\nThe behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n\nFor example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n\n\n  df_original['classID'].replace(['selftalk', 'frustrated', 'delighted', 'dysregulated', 'social', 'request'],\n/tmp/ipykernel_34/808862154.py:2: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n  df_original['classID'].replace(['selftalk', 'frustrated', 'delighted', 'dysregulated', 'social', 'request'],\n","output_type":"stream"}]},{"cell_type":"code","source":"df = df_original\ndf = df.drop(['Participant', 'Label'], axis=1)\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2024-06-21T16:33:32.623255Z","iopub.execute_input":"2024-06-21T16:33:32.623544Z","iopub.status.idle":"2024-06-21T16:33:32.639872Z","shell.execute_reply.started":"2024-06-21T16:33:32.623519Z","shell.execute_reply":"2024-06-21T16:33:32.638701Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"                                    Filename  classID\n0   210408_2025_00-01-38.27--00-01-40.84.wav        4\n1   210324_2036_00-06-03.61--00-06-05.78.wav        4\n2   210324_2036_00-09-04.66--00-09-06.16.wav        4\n3     210324_2036_00-11-18.6--00-11-20.3.wav        4\n4  200506_2110_00-01-25.92--00-01-26.58c.wav        4","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Filename</th>\n      <th>classID</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>210408_2025_00-01-38.27--00-01-40.84.wav</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>210324_2036_00-06-03.61--00-06-05.78.wav</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>210324_2036_00-09-04.66--00-09-06.16.wav</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>210324_2036_00-11-18.6--00-11-20.3.wav</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>200506_2110_00-01-25.92--00-01-26.58c.wav</td>\n      <td>4</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"df['classID'].value_counts() #Number of datapoints available for each target variable","metadata":{"execution":{"iopub.status.busy":"2024-06-21T16:33:32.641261Z","iopub.execute_input":"2024-06-21T16:33:32.641640Z","iopub.status.idle":"2024-06-21T16:33:32.651401Z","shell.execute_reply.started":"2024-06-21T16:33:32.641591Z","shell.execute_reply":"2024-06-21T16:33:32.650391Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"classID\n0    1885\n1    1536\n2    1272\n3     704\n4     634\n5     419\nName: count, dtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"# Construct file path by concatenating fold and file name\ndf['relative_path'] = '/' + df['Filename'].astype(str)\n\n# Take relevant columns\ndf = df[['relative_path', 'classID']]\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2024-06-21T16:33:32.652938Z","iopub.execute_input":"2024-06-21T16:33:32.653301Z","iopub.status.idle":"2024-06-21T16:33:32.669782Z","shell.execute_reply.started":"2024-06-21T16:33:32.653270Z","shell.execute_reply":"2024-06-21T16:33:32.668766Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"                                relative_path  classID\n0   /210408_2025_00-01-38.27--00-01-40.84.wav        4\n1   /210324_2036_00-06-03.61--00-06-05.78.wav        4\n2   /210324_2036_00-09-04.66--00-09-06.16.wav        4\n3     /210324_2036_00-11-18.6--00-11-20.3.wav        4\n4  /200506_2110_00-01-25.92--00-01-26.58c.wav        4","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>relative_path</th>\n      <th>classID</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>/210408_2025_00-01-38.27--00-01-40.84.wav</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>/210324_2036_00-06-03.61--00-06-05.78.wav</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>/210324_2036_00-09-04.66--00-09-06.16.wav</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>/210324_2036_00-11-18.6--00-11-20.3.wav</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>/200506_2110_00-01-25.92--00-01-26.58c.wav</td>\n      <td>4</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"#Random shuffle of rows.\ndf = df. sample(frac=1)\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2024-06-21T16:33:32.670952Z","iopub.execute_input":"2024-06-21T16:33:32.671259Z","iopub.status.idle":"2024-06-21T16:33:32.686562Z","shell.execute_reply.started":"2024-06-21T16:33:32.671234Z","shell.execute_reply":"2024-06-21T16:33:32.685371Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"                                   relative_path  classID\n155    /210408_2025_00-01-46.24--00-01-47.56.wav        4\n5836  /210113_1909_00-25-26.86--00-25-27.57c.wav        1\n820    /200307_2145_00-01-30.84--00-01-31.14.wav        5\n2343   /200124_1828_00-09-49.84--00-09-50.61.wav        3\n3798  /210203_1649_00-06-28.07--00-06-29.26c.wav        0","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>relative_path</th>\n      <th>classID</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>155</th>\n      <td>/210408_2025_00-01-46.24--00-01-47.56.wav</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>5836</th>\n      <td>/210113_1909_00-25-26.86--00-25-27.57c.wav</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>820</th>\n      <td>/200307_2145_00-01-30.84--00-01-31.14.wav</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>2343</th>\n      <td>/200124_1828_00-09-49.84--00-09-50.61.wav</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>3798</th>\n      <td>/210203_1649_00-06-28.07--00-06-29.26c.wav</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"class AudioUtil:\n    @staticmethod\n    def open(audio_file):\n        sig, sr = torchaudio.load(audio_file)\n        return (sig, sr)\n\n    @staticmethod\n    def rechannel(aud, new_channel):\n        sig, sr = aud\n        if (sig.shape[0] == new_channel):\n            return aud\n        if (new_channel == 1):\n            resig = sig[:1, :]\n        else:\n            resig = torch.cat([sig, sig])\n        return (resig, sr)\n\n    @staticmethod\n    def resample(aud, newsr):\n        sig, sr = aud\n        if (sr == newsr):\n            return aud\n        num_channels = sig.shape[0]\n        resig = torchaudio.transforms.Resample(sr, newsr)(sig[:1, :])\n        if (num_channels > 1):\n            retwo = torchaudio.transforms.Resample(sr, newsr)(sig[1:, :])\n            resig = torch.cat([resig, retwo])\n        return (resig, newsr)\n\n    @staticmethod\n    def pad_trunc(aud, max_ms):\n        sig, sr = aud\n        num_rows, sig_len = sig.shape\n        max_len = sr // 1000 * max_ms\n        if (sig_len > max_len):\n            sig = sig[:, :max_len]\n        elif (sig_len < max_len):\n            pad_begin_len = random.randint(0, max_len - sig_len)\n            pad_end_len = max_len - sig_len - pad_begin_len\n            pad_begin = torch.zeros((num_rows, pad_begin_len))\n            pad_end = torch.zeros((num_rows, pad_end_len))\n            sig = torch.cat((pad_begin, sig, pad_end), 1)\n        return (sig, sr)\n\n    @staticmethod\n    def time_shift(aud, shift_limit):\n        sig, sr = aud\n        _, sig_len = sig.shape\n        shift_amt = int(random.random() * shift_limit * sig_len)\n        return (sig.roll(shift_amt), sr)\n\n    @staticmethod\n    def spectro_gram(aud, n_mels=64, n_fft=1024, hop_len=None):\n        sig, sr = aud\n        top_db = 80\n        spec = torchaudio.transforms.MelSpectrogram(sr, n_fft=n_fft, hop_length=hop_len, n_mels=n_mels)(sig)\n        spec = torchaudio.transforms.AmplitudeToDB(top_db=top_db)(spec)\n        return spec\n\n    @staticmethod\n    def spectro_augment(spec, max_mask_pct=0.1, n_freq_masks=1, n_time_masks=1):\n        _, n_mels, n_steps = spec.shape\n        mask_value = spec.mean()\n        aug_spec = spec\n        freq_mask_param = max_mask_pct * n_mels\n        for _ in range(n_freq_masks):\n            aug_spec = torchaudio.transforms.FrequencyMasking(freq_mask_param)(aug_spec)\n        time_mask_param = max_mask_pct * n_steps\n        for _ in range(n_time_masks):\n            aug_spec = torchaudio.transforms.TimeMasking(time_mask_param)(aug_spec)\n        return aug_spec\n\n    @staticmethod\n    def resize(spec, size):\n        resize_transform = transforms.Resize(size)\n        spec = spec.unsqueeze(0)  # Add a batch dimension\n        spec = resize_transform(spec)\n        return spec.squeeze(0)  # Remove the batch dimension\n","metadata":{"execution":{"iopub.status.busy":"2024-06-21T16:33:32.691235Z","iopub.execute_input":"2024-06-21T16:33:32.691549Z","iopub.status.idle":"2024-06-21T16:33:32.712570Z","shell.execute_reply.started":"2024-06-21T16:33:32.691523Z","shell.execute_reply":"2024-06-21T16:33:32.711377Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"class SoundDS(Dataset):\n  def __init__(self, df, data_path):\n    self.df = df\n    self.data_path = str(data_path)\n    self.duration = 4000\n    self.sr = 44100\n    self.channel = 2\n    self.shift_pct = 0.4\n\n  #Number of items in dataset\n  def __len__(self):\n    return len(self.df)\n\n\n  #Get i'th item in dataset\n  def __getitem__(self, idx):\n    #Absolute file path of the audio file = audio directory + relative path\n    audio_file = self.data_path + self.df.loc[idx, 'relative_path']\n    #Get the Class ID\n    class_id = self.df.loc[idx, 'classID']\n\n    aud = AudioUtil.open(audio_file)\n    \"\"\"\n    Some sounds have a higher sample rate, or fewer channels compared to the\n    majority. So make all sounds have the same number of channels and same\n    sample rate. Unless the sample rate is the same, the pad_trunc will still\n    result in arrays of different lengths, even though the sound duration is\n    the same.\n    \"\"\"\n    reaud = AudioUtil.resample(aud, self.sr)\n    rechan = AudioUtil.rechannel(reaud, self.channel)\n\n\n    dur_aud = AudioUtil.pad_trunc(rechan, self.duration)\n    shift_aud = AudioUtil.time_shift(dur_aud, self.shift_pct)\n    sgram = AudioUtil.spectro_gram(shift_aud, n_mels=64, n_fft=1024, hop_len=None)\n    aug_sgram = AudioUtil.spectro_augment(sgram, max_mask_pct=0.1, n_freq_masks=2, n_time_masks=2)\n    resized_sgram = AudioUtil.resize(aug_sgram, (224, 224))\n\n    return resized_sgram, class_id","metadata":{"execution":{"iopub.status.busy":"2024-06-21T16:33:32.714026Z","iopub.execute_input":"2024-06-21T16:33:32.714378Z","iopub.status.idle":"2024-06-21T16:33:32.726277Z","shell.execute_reply.started":"2024-06-21T16:33:32.714348Z","shell.execute_reply":"2024-06-21T16:33:32.725372Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"data_path = \"/kaggle/input/autism-paper-data/Retained/audio\"\nmyds = SoundDS(df, data_path)\n#myds = SoundDS(df, path)\n\n#Random split of 80:20 between training and validation\nnum_items = len(myds)\nnum_train = round(num_items * 0.8)\nnum_val = num_items - num_train\ntrain_ds, val_ds = random_split(myds, [num_train, num_val])\n\n#Create training and validation data loaders\ntrain_dl = torch.utils.data.DataLoader(train_ds, batch_size=16, shuffle=True)\nval_dl = torch.utils.data.DataLoader(val_ds, batch_size=16, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2024-06-21T16:33:32.727697Z","iopub.execute_input":"2024-06-21T16:33:32.728368Z","iopub.status.idle":"2024-06-21T16:33:32.749850Z","shell.execute_reply.started":"2024-06-21T16:33:32.728318Z","shell.execute_reply":"2024-06-21T16:33:32.748999Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"print(num_items, num_train, num_val)","metadata":{"execution":{"iopub.status.busy":"2024-06-21T16:33:32.751151Z","iopub.execute_input":"2024-06-21T16:33:32.751837Z","iopub.status.idle":"2024-06-21T16:33:32.757206Z","shell.execute_reply.started":"2024-06-21T16:33:32.751800Z","shell.execute_reply":"2024-06-21T16:33:32.756129Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"6450 5160 1290\n","output_type":"stream"}]},{"cell_type":"code","source":"type(train_dl)","metadata":{"execution":{"iopub.status.busy":"2024-06-21T16:33:32.758769Z","iopub.execute_input":"2024-06-21T16:33:32.759167Z","iopub.status.idle":"2024-06-21T16:33:32.768730Z","shell.execute_reply.started":"2024-06-21T16:33:32.759132Z","shell.execute_reply":"2024-06-21T16:33:32.767652Z"},"trusted":true},"execution_count":16,"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"torch.utils.data.dataloader.DataLoader"},"metadata":{}}]},{"cell_type":"markdown","source":"# Vision Transformers","metadata":{}},{"cell_type":"code","source":"# Load ViT model from timm library\nmodel = timm.create_model('vit_base_patch16_224', pretrained=True)\n\nmodel.patch_embed.proj = nn.Conv2d(2, model.patch_embed.proj.out_channels, kernel_size=model.patch_embed.proj.kernel_size, stride=model.patch_embed.proj.stride, padding=model.patch_embed.proj.padding)\nnum_ftrs = model.head.in_features\nmodel.head = nn.Linear(num_ftrs, df_original['classID'].nunique())\n\n# Set the device (GPU if available, otherwise CPU)\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\n# Move the model to the device\nmyModel = model.to(device)\n\n# Check that it is on the right device\nprint(next(myModel.parameters()).device)","metadata":{"execution":{"iopub.status.busy":"2024-06-21T16:33:32.769996Z","iopub.execute_input":"2024-06-21T16:33:32.770678Z","iopub.status.idle":"2024-06-21T16:33:36.687821Z","shell.execute_reply.started":"2024-06-21T16:33:32.770639Z","shell.execute_reply":"2024-06-21T16:33:36.686669Z"},"trusted":true},"execution_count":17,"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/346M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"195f3966524245e79a568edc5d265320"}},"metadata":{}},{"name":"stdout","text":"cuda:0\n","output_type":"stream"}]},{"cell_type":"code","source":"def training(model, train_dl, num_epochs):\n  criterion = nn.CrossEntropyLoss()\n  optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n  scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.0001,\n                                                  steps_per_epoch=int(len(train_dl)),\n                                                  epochs=num_epochs,\n                                                  anneal_strategy='linear')\n\n  for epoch in range(num_epochs):\n    running_loss = 0.0\n    correct_prediction = 0\n    total_prediction = 0\n\n    for i, data in enumerate(train_dl):\n      inputs, labels = data[0].to(device), data[1].to(device)\n      inputs_m, inputs_s = inputs.mean(), inputs.std()\n      inputs = (inputs - inputs_m) / inputs_s\n\n      optimizer.zero_grad()\n      outputs = model(inputs)\n      loss = criterion(outputs, labels)\n      loss.backward()\n      optimizer.step()\n      scheduler.step()\n\n      running_loss += loss.item()\n      _, prediction = torch.max(outputs, 1)\n      correct_prediction += (prediction == labels).sum().item()\n      total_prediction += prediction.shape[0]\n\n    avg_loss = running_loss / len(train_dl)\n    acc = correct_prediction / total_prediction\n    print(f'Epoch: {epoch}, Loss: {avg_loss:.2f}, Accuracy: {acc:.2f}')\n\n  print('Finished Training')\n","metadata":{"execution":{"iopub.status.busy":"2024-06-21T16:33:36.688942Z","iopub.execute_input":"2024-06-21T16:33:36.689245Z","iopub.status.idle":"2024-06-21T16:33:36.700088Z","shell.execute_reply.started":"2024-06-21T16:33:36.689219Z","shell.execute_reply":"2024-06-21T16:33:36.698913Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"def inference(model, val_dl):\n  correct_prediction = 0\n  total_prediction = 0\n\n  with torch.no_grad():\n    for data in val_dl:\n      inputs, labels = data[0].to(device), data[1].to(device)\n      inputs_m, inputs_s = inputs.mean(), inputs.std()\n      inputs = (inputs - inputs_m) / inputs_s\n      outputs = model(inputs)\n      _, prediction = torch.max(outputs, 1)\n      correct_prediction += (prediction == labels).sum().item()\n      total_prediction += prediction.shape[0]\n\n  acc = correct_prediction / total_prediction\n  print(f'Accuracy: {acc:.2f}, Total items: {total_prediction}')","metadata":{"execution":{"iopub.status.busy":"2024-06-21T16:33:36.701549Z","iopub.execute_input":"2024-06-21T16:33:36.701929Z","iopub.status.idle":"2024-06-21T16:33:36.712911Z","shell.execute_reply.started":"2024-06-21T16:33:36.701902Z","shell.execute_reply":"2024-06-21T16:33:36.711963Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"print(next(myModel.parameters()).device)","metadata":{"execution":{"iopub.status.busy":"2024-06-21T16:33:36.714061Z","iopub.execute_input":"2024-06-21T16:33:36.714376Z","iopub.status.idle":"2024-06-21T16:33:36.725397Z","shell.execute_reply.started":"2024-06-21T16:33:36.714350Z","shell.execute_reply":"2024-06-21T16:33:36.724414Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"cuda:0\n","output_type":"stream"}]},{"cell_type":"code","source":"num_epochs = 50\ntraining(myModel, train_dl, num_epochs)\ninference(myModel, val_dl)","metadata":{"execution":{"iopub.status.busy":"2024-06-21T16:33:36.726795Z","iopub.execute_input":"2024-06-21T16:33:36.727276Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 0, Loss: 1.65, Accuracy: 0.30\nEpoch: 1, Loss: 1.59, Accuracy: 0.35\nEpoch: 2, Loss: 1.55, Accuracy: 0.38\nEpoch: 3, Loss: 1.51, Accuracy: 0.39\nEpoch: 4, Loss: 1.49, Accuracy: 0.40\nEpoch: 5, Loss: 1.45, Accuracy: 0.41\nEpoch: 6, Loss: 1.41, Accuracy: 0.45\nEpoch: 7, Loss: 1.38, Accuracy: 0.44\nEpoch: 8, Loss: 1.34, Accuracy: 0.48\nEpoch: 9, Loss: 1.33, Accuracy: 0.48\nEpoch: 10, Loss: 1.28, Accuracy: 0.51\nEpoch: 11, Loss: 1.26, Accuracy: 0.52\nEpoch: 12, Loss: 1.22, Accuracy: 0.54\nEpoch: 14, Loss: 1.19, Accuracy: 0.55\nEpoch: 15, Loss: 1.14, Accuracy: 0.56\nEpoch: 16, Loss: 1.12, Accuracy: 0.58\nEpoch: 17, Loss: 1.09, Accuracy: 0.60\nEpoch: 18, Loss: 1.05, Accuracy: 0.61\nEpoch: 19, Loss: 1.01, Accuracy: 0.62\nEpoch: 20, Loss: 0.99, Accuracy: 0.63\nEpoch: 21, Loss: 0.96, Accuracy: 0.64\nEpoch: 22, Loss: 0.91, Accuracy: 0.66\nEpoch: 23, Loss: 0.88, Accuracy: 0.69\nEpoch: 24, Loss: 0.86, Accuracy: 0.68\nEpoch: 25, Loss: 0.83, Accuracy: 0.70\nEpoch: 26, Loss: 0.80, Accuracy: 0.71\nEpoch: 27, Loss: 0.76, Accuracy: 0.72\nEpoch: 28, Loss: 0.72, Accuracy: 0.73\nEpoch: 30, Loss: 0.68, Accuracy: 0.75\nEpoch: 31, Loss: 0.64, Accuracy: 0.76\nEpoch: 32, Loss: 0.62, Accuracy: 0.77\nEpoch: 33, Loss: 0.58, Accuracy: 0.79\nEpoch: 34, Loss: 0.56, Accuracy: 0.80\nEpoch: 36, Loss: 0.49, Accuracy: 0.82\nEpoch: 37, Loss: 0.46, Accuracy: 0.83\nEpoch: 40, Loss: 0.36, Accuracy: 0.87\nEpoch: 42, Loss: 0.29, Accuracy: 0.89\nEpoch: 43, Loss: 0.30, Accuracy: 0.89\nEpoch: 44, Loss: 0.27, Accuracy: 0.90\nEpoch: 45, Loss: 0.25, Accuracy: 0.91\nEpoch: 46, Loss: 0.22, Accuracy: 0.92\nEpoch: 47, Loss: 0.21, Accuracy: 0.92\nEpoch: 48, Loss: 0.19, Accuracy: 0.93\nEpoch: 49, Loss: 0.18, Accuracy: 0.93\nFinished Training\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, f1_score\n\ndef inference(model, val_dl):\n    predictions = []\n    true_labels = []\n    \n    model.eval()\n    with torch.no_grad():\n        for data in val_dl:\n            inputs, labels = data[0].to(device), data[1].to(device)\n            inputs_m, inputs_s = inputs.mean(), inputs.std()\n            inputs = (inputs - inputs_m) / inputs_s\n            outputs = model(inputs)\n            _, prediction = torch.max(outputs, 1)\n            \n            predictions.extend(prediction.tolist())\n            true_labels.extend(labels.tolist())\n\n    accuracy = accuracy_score(true_labels, predictions)\n    f1 = f1_score(true_labels, predictions, average='macro')\n    \n    print(f'Accuracy: {accuracy:.2f}')\n    print(f'F1 Score: {f1:.2f}')\n\ninference(myModel, val_dl)","metadata":{"execution":{"iopub.status.busy":"2024-06-21T19:14:31.061368Z","iopub.execute_input":"2024-06-21T19:14:31.061745Z","iopub.status.idle":"2024-06-21T19:14:59.125877Z","shell.execute_reply.started":"2024-06-21T19:14:31.061713Z","shell.execute_reply":"2024-06-21T19:14:59.124902Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Accuracy: 0.71\nF1 Score: 0.67\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}