{"metadata":{"accelerator":"TPU","colab":{"gpuType":"V28","provenance":[]},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7405399,"sourceType":"datasetVersion","datasetId":4306525},{"sourceId":8588058,"sourceType":"datasetVersion","datasetId":5136800}],"dockerImageVersionId":30716,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"papermill":{"default_parameters":{},"duration":2045.448236,"end_time":"2024-01-16T13:24:39.992706","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-01-16T12:50:34.544470","version":"2.4.0"}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**Dataset Information: -**\n\n\n1.   https://www.spectrumnews.org/news/researchers-publish-new-dataset-on-minimally-verbal-autistic-people/\n2.   https://zenodo.org/record/5786860#.ZG0w_HZBw2w\n\n","metadata":{"id":"a3bdfb5b"}},{"cell_type":"markdown","source":"**Installing files from Zenodo: -**\n\n\n1.   pip install zenodo-get\n2.   zenodo_get 10.5281/zenodo.5786860\n\n(10.5281/zenodo.5786860 is the DOI of the database publication)\n\n\n","metadata":{"id":"091fa2ce"}},{"cell_type":"markdown","source":"#Imports","metadata":{"id":"60528feb"}},{"cell_type":"code","source":"#%tensorflow_version 2.x  # this line is not required unless you are in a notebook\nimport tensorflow as tf\nfrom tensorflow.keras import datasets, layers, models\nimport matplotlib.pyplot as plt","metadata":{"id":"e7853369","execution":{"iopub.status.busy":"2024-06-05T16:13:50.876861Z","iopub.execute_input":"2024-06-05T16:13:50.877295Z","iopub.status.idle":"2024-06-05T16:13:50.882426Z","shell.execute_reply.started":"2024-06-05T16:13:50.877264Z","shell.execute_reply":"2024-06-05T16:13:50.881306Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport math, random\nimport torch\nimport torchaudio\nfrom torchaudio import transforms\nfrom IPython.display import Audio","metadata":{"id":"381f4d4c","execution":{"iopub.status.busy":"2024-06-05T16:13:52.256337Z","iopub.execute_input":"2024-06-05T16:13:52.257222Z","iopub.status.idle":"2024-06-05T16:13:56.278736Z","shell.execute_reply.started":"2024-06-05T16:13:52.257189Z","shell.execute_reply":"2024-06-05T16:13:56.277703Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import DataLoader, Dataset, random_split\nimport torchaudio\nimport torch.nn.functional as F\nfrom torch.nn import init\nimport torch\nimport torchvision\nimport torch.nn as nn","metadata":{"id":"6fc4ea4b","execution":{"iopub.status.busy":"2024-06-05T16:13:56.280528Z","iopub.execute_input":"2024-06-05T16:13:56.281254Z","iopub.status.idle":"2024-06-05T16:13:57.924964Z","shell.execute_reply.started":"2024-06-05T16:13:56.281226Z","shell.execute_reply":"2024-06-05T16:13:57.923926Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"#Read and Prepare Dataset\n","metadata":{"id":"71202ee6"}},{"cell_type":"code","source":"path = \"/kaggle/input/autism-paper-data/Retained/dataset_file_directory.csv\"\ndf_original = pd.read_csv(path)\ndf_original.head()\n\n#There are 6 classes of sounds in the dataset.\n#The class label is categorical, and hence will be converted to a numeric Class ID later.\n#E.g.: 0 = air conditioner, 1 = car horn, etc.","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":224},"id":"93f8b2df","outputId":"ced96494-c6cb-4675-eddf-16c6427b1e65","execution":{"iopub.status.busy":"2024-06-05T16:13:57.926216Z","iopub.execute_input":"2024-06-05T16:13:57.926602Z","iopub.status.idle":"2024-06-05T16:13:57.977417Z","shell.execute_reply.started":"2024-06-05T16:13:57.926566Z","shell.execute_reply":"2024-06-05T16:13:57.976495Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"                                    Filename Participant   Label\n0   210408_2025_00-01-38.27--00-01-40.84.wav         P01  social\n1   210324_2036_00-06-03.61--00-06-05.78.wav         P01  social\n2   210324_2036_00-09-04.66--00-09-06.16.wav         P01  social\n3     210324_2036_00-11-18.6--00-11-20.3.wav         P01  social\n4  200506_2110_00-01-25.92--00-01-26.58c.wav         P01  social","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Filename</th>\n      <th>Participant</th>\n      <th>Label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>210408_2025_00-01-38.27--00-01-40.84.wav</td>\n      <td>P01</td>\n      <td>social</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>210324_2036_00-06-03.61--00-06-05.78.wav</td>\n      <td>P01</td>\n      <td>social</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>210324_2036_00-09-04.66--00-09-06.16.wav</td>\n      <td>P01</td>\n      <td>social</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>210324_2036_00-11-18.6--00-11-20.3.wav</td>\n      <td>P01</td>\n      <td>social</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>200506_2110_00-01-25.92--00-01-26.58c.wav</td>\n      <td>P01</td>\n      <td>social</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"Training Data: -\n*   The features (X): Audio Paths\n*   The target labels (y): Class Names","metadata":{"id":"17871aac"}},{"cell_type":"code","source":"df_original.shape[0]","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dc9ac37d","outputId":"5f3871fe-984d-4102-ad7e-3710266634d0","execution":{"iopub.status.busy":"2024-06-05T16:13:57.980208Z","iopub.execute_input":"2024-06-05T16:13:57.980596Z","iopub.status.idle":"2024-06-05T16:13:57.986886Z","shell.execute_reply.started":"2024-06-05T16:13:57.980563Z","shell.execute_reply":"2024-06-05T16:13:57.985939Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"6450"},"metadata":{}}]},{"cell_type":"code","source":"df_original.Label.unique() #Print unique types of sound","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"23250207","outputId":"f9309dff-1e66-482c-cf11-561144e78eb6","execution":{"iopub.status.busy":"2024-06-05T16:13:57.988233Z","iopub.execute_input":"2024-06-05T16:13:57.989045Z","iopub.status.idle":"2024-06-05T16:13:58.001704Z","shell.execute_reply.started":"2024-06-05T16:13:57.989014Z","shell.execute_reply":"2024-06-05T16:13:58.000606Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"array(['social', 'selftalk', 'request', 'delighted', 'dysregulated',\n       'frustrated'], dtype=object)"},"metadata":{}}]},{"cell_type":"code","source":"df_original['Label'].nunique() #Our dataset has 22 target variables/classes.","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2a94ea9d","outputId":"f42852e5-b41a-4d24-e949-fb82677a877c","execution":{"iopub.status.busy":"2024-06-05T16:13:58.003331Z","iopub.execute_input":"2024-06-05T16:13:58.003842Z","iopub.status.idle":"2024-06-05T16:13:58.013775Z","shell.execute_reply.started":"2024-06-05T16:13:58.003813Z","shell.execute_reply":"2024-06-05T16:13:58.012670Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"6"},"metadata":{}}]},{"cell_type":"code","source":"df_original['Label'].value_counts() #Number of datapoints available for each target variable","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"89a0535d","outputId":"32a5a6f0-5d68-48d9-a774-a92dc0c206fa","execution":{"iopub.status.busy":"2024-06-05T16:13:58.014893Z","iopub.execute_input":"2024-06-05T16:13:58.015769Z","iopub.status.idle":"2024-06-05T16:13:58.035227Z","shell.execute_reply.started":"2024-06-05T16:13:58.015718Z","shell.execute_reply":"2024-06-05T16:13:58.034170Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"Label\nselftalk        1885\nfrustrated      1536\ndelighted       1272\ndysregulated     704\nsocial           634\nrequest          419\nName: count, dtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"#Idea: Consider only the top 6 majority classes and drop the rest to account for class imbalance.\n#Problem: Path (file) names are random, and they are not foldered/organzied. Hence we need to write a code that will read the csv file after deletion, and pick only the un-deleted ones from the audio folder.\n\"\"\"\n#drop minority classes\nindex_names = df_original[ df_original['Label'] == \"greeting\" ].index\ndf_original.drop(index_names, inplace = True)\n\nindex_names = df_original[ df_original['Label'] == \"hunger\" ].index\ndf_original.drop(index_names, inplace = True)\n\nindex_names = df_original[ df_original['Label'] == \"tablet\" ].index\ndf_original.drop(index_names, inplace = True)\n\nindex_names = df_original[ df_original['Label'] == \"laugh\" ].index\ndf_original.drop(index_names, inplace = True)\n\nindex_names = df_original[ df_original['Label'] == \"glee\" ].index\ndf_original.drop(index_names, inplace = True)\n\nindex_names = df_original[ df_original['Label'] == \"no\" ].index\ndf_original.drop(index_names, inplace = True)\n\nindex_names = df_original[ df_original['Label'] == \"dysregulation-bathroom\" ].index\ndf_original.drop(index_names, inplace = True)\n\nindex_names = df_original[ df_original['Label'] == \"bathroom\" ].index\ndf_original.drop(index_names, inplace = True)\n\nindex_names = df_original[ df_original['Label'] == \"protest\" ].index\ndf_original.drop(index_names, inplace = True)\n\nindex_names = df_original[ df_original['Label'] == \"more\" ].index\ndf_original.drop(index_names, inplace = True)\n\nindex_names = df_original[ df_original['Label'] == \"help\" ].index\ndf_original.drop(index_names, inplace = True)\n\nindex_names = df_original[ df_original['Label'] == \"happy\" ].index\ndf_original.drop(index_names, inplace = True)\n\nindex_names = df_original[ df_original['Label'] == \"dysregulation-sick\" ].index\ndf_original.drop(index_names, inplace = True)\n\nindex_names = df_original[ df_original['Label'] == \"yes\" ].index\ndf_original.drop(index_names, inplace = True)\n\nindex_names = df_original[ df_original['Label'] == \"affectionate\" ].index\ndf_original.drop(index_names, inplace = True)\n\nindex_names = df_original[ df_original['Label'] == \"laughter\" ].index\ndf_original.drop(index_names, inplace = True)\n\"\"\"","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":178},"id":"a3de0f25","outputId":"bc8a6268-0bab-416d-b71d-ff31793458b6","execution":{"iopub.status.busy":"2024-06-05T16:13:58.036455Z","iopub.execute_input":"2024-06-05T16:13:58.036831Z","iopub.status.idle":"2024-06-05T16:13:58.048810Z","shell.execute_reply.started":"2024-06-05T16:13:58.036801Z","shell.execute_reply":"2024-06-05T16:13:58.047912Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"'\\n#drop minority classes\\nindex_names = df_original[ df_original[\\'Label\\'] == \"greeting\" ].index\\ndf_original.drop(index_names, inplace = True)\\n\\nindex_names = df_original[ df_original[\\'Label\\'] == \"hunger\" ].index\\ndf_original.drop(index_names, inplace = True)\\n\\nindex_names = df_original[ df_original[\\'Label\\'] == \"tablet\" ].index\\ndf_original.drop(index_names, inplace = True)\\n\\nindex_names = df_original[ df_original[\\'Label\\'] == \"laugh\" ].index\\ndf_original.drop(index_names, inplace = True)\\n\\nindex_names = df_original[ df_original[\\'Label\\'] == \"glee\" ].index\\ndf_original.drop(index_names, inplace = True)\\n\\nindex_names = df_original[ df_original[\\'Label\\'] == \"no\" ].index\\ndf_original.drop(index_names, inplace = True)\\n\\nindex_names = df_original[ df_original[\\'Label\\'] == \"dysregulation-bathroom\" ].index\\ndf_original.drop(index_names, inplace = True)\\n\\nindex_names = df_original[ df_original[\\'Label\\'] == \"bathroom\" ].index\\ndf_original.drop(index_names, inplace = True)\\n\\nindex_names = df_original[ df_original[\\'Label\\'] == \"protest\" ].index\\ndf_original.drop(index_names, inplace = True)\\n\\nindex_names = df_original[ df_original[\\'Label\\'] == \"more\" ].index\\ndf_original.drop(index_names, inplace = True)\\n\\nindex_names = df_original[ df_original[\\'Label\\'] == \"help\" ].index\\ndf_original.drop(index_names, inplace = True)\\n\\nindex_names = df_original[ df_original[\\'Label\\'] == \"happy\" ].index\\ndf_original.drop(index_names, inplace = True)\\n\\nindex_names = df_original[ df_original[\\'Label\\'] == \"dysregulation-sick\" ].index\\ndf_original.drop(index_names, inplace = True)\\n\\nindex_names = df_original[ df_original[\\'Label\\'] == \"yes\" ].index\\ndf_original.drop(index_names, inplace = True)\\n\\nindex_names = df_original[ df_original[\\'Label\\'] == \"affectionate\" ].index\\ndf_original.drop(index_names, inplace = True)\\n\\nindex_names = df_original[ df_original[\\'Label\\'] == \"laughter\" ].index\\ndf_original.drop(index_names, inplace = True)\\n'"},"metadata":{}}]},{"cell_type":"code","source":"#Convert Text to number notation for target variable (Categorical to numerical data)\n\n\"\"\"\nCombine classes to account for class imbalance and to reduce the number of classes: -\nself talk - 0\nfrusturated, protest, no - 1\ndelighted, laughter, happy, glee, laugh  - 2\ndysregulated, dysregulation-sick, dysregulation-bathroom - 3\nsocial, affectionate, greeting - 4\nrequest, help, bathroom, tablet, bathroom - 5\nyes, more, hunger - 6\n\"\"\"\n\ndf_original[\"classID\"] = df_original['Label']\ndf_original['classID'].replace(['selftalk', 'frustrated', 'delighted', 'dysregulated', 'social', 'request'],\n                        [0, 1, 2, 3, 4, 5], inplace=True)\n\n\n#Create new dataframe called 'df' with only two columns - Filename, classID.\ndf = df_original\ndf = df.drop(['Participant', 'Label'], axis=1)\ndf.head()","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"dd398af2","outputId":"d7f50135-3278-4247-bcdc-d6fdd9d81395","execution":{"iopub.status.busy":"2024-06-05T16:13:58.049966Z","iopub.execute_input":"2024-06-05T16:13:58.050238Z","iopub.status.idle":"2024-06-05T16:13:58.088140Z","shell.execute_reply.started":"2024-06-05T16:13:58.050216Z","shell.execute_reply":"2024-06-05T16:13:58.087018Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_35/4223752362.py:15: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\nThe behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n\nFor example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n\n\n  df_original['classID'].replace(['selftalk', 'frustrated', 'delighted', 'dysregulated', 'social', 'request'],\n/tmp/ipykernel_35/4223752362.py:15: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n  df_original['classID'].replace(['selftalk', 'frustrated', 'delighted', 'dysregulated', 'social', 'request'],\n","output_type":"stream"},{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"                                    Filename  classID\n0   210408_2025_00-01-38.27--00-01-40.84.wav        4\n1   210324_2036_00-06-03.61--00-06-05.78.wav        4\n2   210324_2036_00-09-04.66--00-09-06.16.wav        4\n3     210324_2036_00-11-18.6--00-11-20.3.wav        4\n4  200506_2110_00-01-25.92--00-01-26.58c.wav        4","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Filename</th>\n      <th>classID</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>210408_2025_00-01-38.27--00-01-40.84.wav</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>210324_2036_00-06-03.61--00-06-05.78.wav</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>210324_2036_00-09-04.66--00-09-06.16.wav</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>210324_2036_00-11-18.6--00-11-20.3.wav</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>200506_2110_00-01-25.92--00-01-26.58c.wav</td>\n      <td>4</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"df['classID'].value_counts() #Number of datapoints available for each target variable","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"58f7a951","outputId":"b52d1d70-6887-4df6-a297-16b257336c4c","execution":{"iopub.status.busy":"2024-06-05T16:13:58.091998Z","iopub.execute_input":"2024-06-05T16:13:58.092416Z","iopub.status.idle":"2024-06-05T16:13:58.102942Z","shell.execute_reply.started":"2024-06-05T16:13:58.092380Z","shell.execute_reply":"2024-06-05T16:13:58.101505Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"classID\n0    1885\n1    1536\n2    1272\n3     704\n4     634\n5     419\nName: count, dtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"# Construct file path by concatenating fold and file name\ndf['relative_path'] = '/' + df['Filename'].astype(str)\n\n# Take relevant columns\ndf = df[['relative_path', 'classID']]\ndf.head()","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"5eeaedbc","outputId":"2c6859f5-b969-4f86-e409-2d278dd8e41f","execution":{"iopub.status.busy":"2024-06-05T16:13:58.104530Z","iopub.execute_input":"2024-06-05T16:13:58.104879Z","iopub.status.idle":"2024-06-05T16:13:58.126326Z","shell.execute_reply.started":"2024-06-05T16:13:58.104848Z","shell.execute_reply":"2024-06-05T16:13:58.125339Z"},"trusted":true},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"                                relative_path  classID\n0   /210408_2025_00-01-38.27--00-01-40.84.wav        4\n1   /210324_2036_00-06-03.61--00-06-05.78.wav        4\n2   /210324_2036_00-09-04.66--00-09-06.16.wav        4\n3     /210324_2036_00-11-18.6--00-11-20.3.wav        4\n4  /200506_2110_00-01-25.92--00-01-26.58c.wav        4","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>relative_path</th>\n      <th>classID</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>/210408_2025_00-01-38.27--00-01-40.84.wav</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>/210324_2036_00-06-03.61--00-06-05.78.wav</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>/210324_2036_00-09-04.66--00-09-06.16.wav</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>/210324_2036_00-11-18.6--00-11-20.3.wav</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>/200506_2110_00-01-25.92--00-01-26.58c.wav</td>\n      <td>4</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"#Audio Pre-processing","metadata":{"id":"ee0fa2a4"}},{"cell_type":"markdown","source":"Audio data is memory intensive. So, we don’t want to read the entire dataset into memory all at once. Instead, we keep only the audio file names in our training data and perform pre-processing dynamically during runtime.","metadata":{"id":"9c236bd5"}},{"cell_type":"markdown","source":"Then, at runtime, as we train the model one batch at a time, we will load the audio data for that batch and process it by applying a series of transforms to the audio. That way we keep audio data for only one batch in memory at a time.","metadata":{"id":"6e40b24e"}},{"cell_type":"code","source":"#Random shuffle of rows.\ndf = df. sample(frac=1)\ndf.head()","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"8ee7bf8b","outputId":"b8a96d12-f790-46d8-ae71-7a6665d0bdde","execution":{"iopub.status.busy":"2024-06-05T16:13:58.127687Z","iopub.execute_input":"2024-06-05T16:13:58.128039Z","iopub.status.idle":"2024-06-05T16:13:58.141517Z","shell.execute_reply.started":"2024-06-05T16:13:58.128005Z","shell.execute_reply":"2024-06-05T16:13:58.140423Z"},"trusted":true},"execution_count":15,"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"                                    relative_path  classID\n2725    /200605_1745_01-43-34.06--01-43-35.67.wav        3\n498   /200329_1113_00-14-06.278--00-14-07.056.wav        0\n3131   /200604_1725_00-12-41.29--00-12-42.51c.wav        0\n2370     /200411_1548_00-55-01.84--00-55-04.2.wav        3\n203     /200306_2024_00-09-52.54--00-09-52.98.wav        0","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>relative_path</th>\n      <th>classID</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2725</th>\n      <td>/200605_1745_01-43-34.06--01-43-35.67.wav</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>498</th>\n      <td>/200329_1113_00-14-06.278--00-14-07.056.wav</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3131</th>\n      <td>/200604_1725_00-12-41.29--00-12-42.51c.wav</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2370</th>\n      <td>/200411_1548_00-55-01.84--00-55-04.2.wav</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>203</th>\n      <td>/200306_2024_00-09-52.54--00-09-52.98.wav</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"#Function to read and load the audio files in '.wav' format using Pytorch.\nclass AudioUtil():\n  @staticmethod\n  def open(audio_file):\n    sig, sr = torchaudio.load(audio_file) # Load an audio file. Return the signal as a tensor and the sample rate\n    return (sig, sr)\n\n\n#Refer tutotial file for explaination about the following blocks of code: -\n  def rechannel(aud, new_channel):\n    sig, sr = aud\n    if (sig.shape[0] == new_channel):\n      return aud\n    if (new_channel == 1):\n      resig = sig[:1, :]\n    else:\n      resig = torch.cat([sig, sig])\n    return ((resig, sr))\n\n  def resample(aud, newsr):\n    sig, sr = aud\n    if (sr == newsr):\n      return aud\n    num_channels = sig.shape[0]\n    resig = torchaudio.transforms.Resample(sr, newsr)(sig[:1,:])\n    if (num_channels > 1):\n      retwo = torchaudio.transforms.Resample(sr, newsr)(sig[1:,:])\n      resig = torch.cat([resig, retwo])\n    return ((resig, newsr))\n\n  def pad_trunc(aud, max_ms):\n    sig, sr = aud\n    num_rows, sig_len = sig.shape\n    max_len = sr//1000 * max_ms\n    if (sig_len > max_len):\n      sig = sig[:,:max_len]\n    elif (sig_len < max_len):\n      pad_begin_len = random.randint(0, max_len - sig_len)\n      pad_end_len = max_len - sig_len - pad_begin_len\n      pad_begin = torch.zeros((num_rows, pad_begin_len))\n      pad_end = torch.zeros((num_rows, pad_end_len))\n      sig = torch.cat((pad_begin, sig, pad_end), 1)\n    return (sig, sr)\n\n  def time_shift(aud, shift_limit):\n    sig,sr = aud\n    _, sig_len = sig.shape\n    shift_amt = int(random.random() * shift_limit * sig_len)\n    return (sig.roll(shift_amt), sr)\n\n  def spectro_gram(aud, n_mels=64, n_fft=1024, hop_len=None):\n    sig,sr = aud\n    top_db = 80\n    spec = transforms.MelSpectrogram(sr, n_fft=n_fft, hop_length=hop_len, n_mels=n_mels)(sig)\n    spec = transforms.AmplitudeToDB(top_db=top_db)(spec)\n    return (spec)\n\n  def spectro_augment(spec, max_mask_pct=0.1, n_freq_masks=1, n_time_masks=1):\n    _, n_mels, n_steps = spec.shape\n    mask_value = spec.mean()\n    aug_spec = spec\n    freq_mask_param = max_mask_pct * n_mels\n    for _ in range(n_freq_masks):\n      aug_spec = transforms.FrequencyMasking(freq_mask_param)(aug_spec, mask_value)\n    time_mask_param = max_mask_pct * n_steps\n    for _ in range(n_time_masks):\n      aug_spec = transforms.TimeMasking(time_mask_param)(aug_spec, mask_value)\n    return aug_spec","metadata":{"id":"852bed24","execution":{"iopub.status.busy":"2024-06-05T16:13:58.142931Z","iopub.execute_input":"2024-06-05T16:13:58.143725Z","iopub.status.idle":"2024-06-05T16:13:58.159219Z","shell.execute_reply.started":"2024-06-05T16:13:58.143701Z","shell.execute_reply":"2024-06-05T16:13:58.158436Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"#Define Custom Data Loader\n","metadata":{"id":"8bd3a4ab"}},{"cell_type":"markdown","source":"To feed the data to a model with Pytorch, we need two objects:\n\n*   A custom Dataset object that uses all the audio transforms to pre-process an audio file and prepare one data item at a time.\n*   A built-in DataLoader object that uses the Dataset object to fetch individual data items and packages them into a batch of data.","metadata":{"id":"377be9fe"}},{"cell_type":"code","source":"class SoundDS(Dataset):\n  def __init__(self, df, data_path):\n    self.df = df\n    self.data_path = str(data_path)\n    self.duration = 4000\n    self.sr = 44100\n    self.channel = 2\n    self.shift_pct = 0.4\n\n  #Number of items in dataset\n  def __len__(self):\n    return len(self.df)\n\n\n  #Get i'th item in dataset\n  def __getitem__(self, idx):\n    #Absolute file path of the audio file = audio directory + relative path\n    audio_file = self.data_path + self.df.loc[idx, 'relative_path']\n    #Get the Class ID\n    class_id = self.df.loc[idx, 'classID']\n\n    aud = AudioUtil.open(audio_file)\n    \"\"\"\n    Some sounds have a higher sample rate, or fewer channels compared to the\n    majority. So make all sounds have the same number of channels and same\n    sample rate. Unless the sample rate is the same, the pad_trunc will still\n    result in arrays of different lengths, even though the sound duration is\n    the same.\n    \"\"\"\n    reaud = AudioUtil.resample(aud, self.sr)\n    rechan = AudioUtil.rechannel(reaud, self.channel)\n\n\n    dur_aud = AudioUtil.pad_trunc(rechan, self.duration)\n    shift_aud = AudioUtil.time_shift(dur_aud, self.shift_pct)\n    sgram = AudioUtil.spectro_gram(shift_aud, n_mels=64, n_fft=1024, hop_len=None)\n    aug_sgram = AudioUtil.spectro_augment(sgram, max_mask_pct=0.1, n_freq_masks=2, n_time_masks=2)\n\n    return aug_sgram, class_id","metadata":{"id":"09362a9e","execution":{"iopub.status.busy":"2024-06-05T16:13:58.160686Z","iopub.execute_input":"2024-06-05T16:13:58.161038Z","iopub.status.idle":"2024-06-05T16:13:58.178953Z","shell.execute_reply.started":"2024-06-05T16:13:58.161009Z","shell.execute_reply":"2024-06-05T16:13:58.177937Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"#Prepare Batches of Data with the Data Loader","metadata":{"id":"40990d23"}},{"cell_type":"markdown","source":"Load the Features and Labels from our Pandas dataframe and split it in an 80:20 ratio for training and validation. Then, use them to create the training and validation Data Loaders.","metadata":{"id":"05e2ade1"}},{"cell_type":"code","source":"data_path = \"/kaggle/input/autism-paper-data/Retained/audio\"\nmyds = SoundDS(df, data_path)\n#myds = SoundDS(df, path)\n\n#Random split of 80:20 between training and validation\nnum_items = len(myds)\nnum_train = round(num_items * 0.8)\nnum_val = num_items - num_train\ntrain_ds, val_ds = random_split(myds, [num_train, num_val])\n\n#Create training and validation data loaders\ntrain_dl = torch.utils.data.DataLoader(train_ds, batch_size=16, shuffle=True)\nval_dl = torch.utils.data.DataLoader(val_ds, batch_size=16, shuffle=False)","metadata":{"id":"4de0bb5c","execution":{"iopub.status.busy":"2024-06-05T16:13:58.181686Z","iopub.execute_input":"2024-06-05T16:13:58.182129Z","iopub.status.idle":"2024-06-05T16:13:58.211524Z","shell.execute_reply.started":"2024-06-05T16:13:58.182082Z","shell.execute_reply":"2024-06-05T16:13:58.210517Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"print(num_items, num_train, num_val)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"af0a0d53","outputId":"fabe0229-3ab6-494c-bc43-ab32341bf4d2","execution":{"iopub.status.busy":"2024-06-05T16:13:58.213023Z","iopub.execute_input":"2024-06-05T16:13:58.213988Z","iopub.status.idle":"2024-06-05T16:13:58.219438Z","shell.execute_reply.started":"2024-06-05T16:13:58.213952Z","shell.execute_reply":"2024-06-05T16:13:58.218244Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"6450 5160 1290\n","output_type":"stream"}]},{"cell_type":"code","source":"type(train_dl)","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":291},"id":"52fcf755","outputId":"04f41c0b-7bb7-440d-d287-c5926b72695f","execution":{"iopub.status.busy":"2024-06-05T16:13:58.220954Z","iopub.execute_input":"2024-06-05T16:13:58.221326Z","iopub.status.idle":"2024-06-05T16:13:58.230416Z","shell.execute_reply.started":"2024-06-05T16:13:58.221292Z","shell.execute_reply":"2024-06-05T16:13:58.229525Z"},"trusted":true},"execution_count":20,"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"torch.utils.data.dataloader.DataLoader"},"metadata":{}}]},{"cell_type":"markdown","source":"During training, the Data Loader will randomly fetch one batch of input features containing the list of audio file names and run the pre-processing audio transforms on each audio file. It will also fetch the corresponding target labels containing the class IDs. Thus it will output one batch of training data at a time, which can directly be fed as input to our DL model.","metadata":{"id":"089e2e7d"}},{"cell_type":"markdown","source":"\n\n*   The audio from the file gets loaded into a Numpy array of shape (num_channels, num_samples). Most of the audio is sampled at 44.1kHz and is about 4 seconds in duration, resulting in 44,100 * 4 = 176,400 samples. If the audio has 1 channel, the shape of the array will be (1, 176400). Similarly, audio of 4 seconds duration with 2 channels and sampled at 48kHz will have 192,000 samples and a shape of (2, 192000).\n*   Since the channels and sampling rates of each audio are different, the next two transforms resample the audio to a standard 44.1kHz and to a standard 2 channels.\n*   Since some audio clips might be more or less than 4 seconds, we also standardize the audio duration to a fixed length of 4 seconds. Now arrays for all items have the same shape of (2, 176,400).\n*   The Time Shift data augmentation now randomly shifts each audio sample forward or backward. The shapes are unchanged.\n*   The augmented audio is now converted into a Mel Spectrogram, resulting in a shape of (num_channels, Mel freq_bands, time_steps) = (2, 64, 344).\n*   The SpecAugment data augmentation now randomly applies Time and Frequency Masks to the Mel Spectrograms. The shapes are unchanged.\n\nThus, each batch will have two tensors, one for the X feature data containing the Mel Spectrograms and the other for the y target labels containing numeric Class IDs. The batches are picked randomly from the training data for each training epoch.\nEach batch has a shape of (batch_sz, num_channels, Mel freq_bands, time_steps).\n\n\n\n\n","metadata":{"id":"172f3aef"}},{"cell_type":"markdown","source":"# Alex Net","metadata":{}},{"cell_type":"code","source":"\"\"\"\nimport torchvision.models as models\n\n# Load pretrained AlexNet\nmodel = models.alexnet(pretrained=True)\n\n# Modify the first layer to accept 2-channel input (for spectrograms)\nmodel.features[0] = nn.Conv2d(2, model.features[0].out_channels,\n                              kernel_size=model.features[0].kernel_size,\n                              stride=model.features[0].stride,\n                              padding=model.features[0].padding)\n\n# Modify the last layer to match the number of output classes\nnum_ftrs = model.classifier[-1].in_features\nmodel.classifier[-1] = nn.Linear(num_ftrs, 6)  # Adjust the number of classes accordingly\n\n# Move the model to GPU if available\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nmyModel = model.to(device)\n\n# Check that it is on Cuda\nnext(myModel.parameters()).device\n\"\"\"","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ba2cc1f1","outputId":"9d46652d-7030-42b5-c092-b891c9f7ca3c","execution":{"iopub.status.busy":"2024-06-04T19:03:49.955649Z","iopub.execute_input":"2024-06-04T19:03:49.956080Z","iopub.status.idle":"2024-06-04T19:03:52.478705Z","shell.execute_reply.started":"2024-06-04T19:03:49.956049Z","shell.execute_reply":"2024-06-04T19:03:52.477807Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/alexnet-owt-7be5be79.pth\" to /root/.cache/torch/hub/checkpoints/alexnet-owt-7be5be79.pth\n100%|██████████| 233M/233M [00:01<00:00, 174MB/s]  \n","output_type":"stream"},{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"device(type='cuda', index=0)"},"metadata":{}}]},{"cell_type":"code","source":"# Define training function\ndef training(model, train_dl, num_epochs):\n    criterion = nn.CrossEntropyLoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n    scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.0001,\n                                                    steps_per_epoch=int(len(train_dl)),\n                                                    epochs=num_epochs,\n                                                    anneal_strategy='linear')\n\n    for epoch in range(num_epochs):\n        running_loss = 0.0\n        correct_prediction = 0\n        \n        total_prediction = 0\n\n        for i, data in enumerate(train_dl):\n            inputs, labels = data[0].to(device), data[1].to(device)\n\n            inputs_m, inputs_s = inputs.mean(), inputs.std()\n            inputs = (inputs - inputs_m) / inputs_s\n\n            optimizer.zero_grad()\n\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            scheduler.step()\n\n            running_loss += loss.item()\n\n            _, prediction = torch.max(outputs, 1)\n            correct_prediction += (prediction == labels).sum().item()\n            total_prediction += prediction.shape[0]\n\n        avg_loss = running_loss / len(train_dl)\n        acc = correct_prediction / total_prediction\n        print(f'Epoch: {epoch}, Loss: {avg_loss:.2f}, Accuracy: {acc:.2f}')\n\n    print('Finished Training')\n","metadata":{"id":"kVmPRj0xKnfa","execution":{"iopub.status.busy":"2024-06-05T15:04:40.662602Z","iopub.execute_input":"2024-06-05T15:04:40.663469Z","iopub.status.idle":"2024-06-05T15:04:40.672948Z","shell.execute_reply.started":"2024-06-05T15:04:40.663437Z","shell.execute_reply":"2024-06-05T15:04:40.671805Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"# Define inference function\ndef inference(model, val_dl):\n    correct_prediction = 0\n    total_prediction = 0\n\n    with torch.no_grad():\n        for data in val_dl:\n            inputs, labels = data[0].to(device), data[1].to(device)\n\n            inputs_m, inputs_s = inputs.mean(), inputs.std()\n            inputs = (inputs - inputs_m) / inputs_s\n\n            outputs = model(inputs)\n\n            _, prediction = torch.max(outputs, 1)\n            correct_prediction += (prediction == labels).sum().item()\n            total_prediction += prediction.shape[0]\n\n    acc = correct_prediction / total_prediction\n    print(f'Accuracy: {acc:.2f}, Total items: {total_prediction}')","metadata":{"id":"GqDfOKyMKnpX","execution":{"iopub.status.busy":"2024-06-05T15:04:41.523624Z","iopub.execute_input":"2024-06-05T15:04:41.524333Z","iopub.status.idle":"2024-06-05T15:04:41.532329Z","shell.execute_reply.started":"2024-06-05T15:04:41.524296Z","shell.execute_reply":"2024-06-05T15:04:41.531365Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"# Train the model\nnum_epochs = 50\ntraining(myModel, train_dl, num_epochs)\n\n# Run inference on the trained model\ninference(myModel, val_dl)","metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"hxCQlo4VKso4","outputId":"9cd842fc-2f27-4d2b-bc11-36f4c379172f","execution":{"iopub.status.busy":"2024-06-04T19:03:52.505964Z","iopub.execute_input":"2024-06-04T19:03:52.506231Z","iopub.status.idle":"2024-06-04T20:09:03.330627Z","shell.execute_reply.started":"2024-06-04T19:03:52.506209Z","shell.execute_reply":"2024-06-04T20:09:03.329718Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"Epoch: 0, Loss: 1.66, Accuracy: 0.30\nEpoch: 1, Loss: 1.59, Accuracy: 0.34\nEpoch: 2, Loss: 1.55, Accuracy: 0.37\nEpoch: 3, Loss: 1.51, Accuracy: 0.38\nEpoch: 4, Loss: 1.47, Accuracy: 0.40\nEpoch: 5, Loss: 1.44, Accuracy: 0.42\nEpoch: 6, Loss: 1.41, Accuracy: 0.44\nEpoch: 7, Loss: 1.38, Accuracy: 0.45\nEpoch: 8, Loss: 1.38, Accuracy: 0.45\nEpoch: 9, Loss: 1.33, Accuracy: 0.49\nEpoch: 10, Loss: 1.33, Accuracy: 0.48\nEpoch: 11, Loss: 1.32, Accuracy: 0.49\nEpoch: 12, Loss: 1.29, Accuracy: 0.50\nEpoch: 13, Loss: 1.29, Accuracy: 0.50\nEpoch: 14, Loss: 1.26, Accuracy: 0.51\nEpoch: 15, Loss: 1.24, Accuracy: 0.52\nEpoch: 16, Loss: 1.21, Accuracy: 0.54\nEpoch: 17, Loss: 1.19, Accuracy: 0.55\nEpoch: 18, Loss: 1.18, Accuracy: 0.55\nEpoch: 19, Loss: 1.16, Accuracy: 0.56\nEpoch: 20, Loss: 1.14, Accuracy: 0.57\nEpoch: 21, Loss: 1.11, Accuracy: 0.58\nEpoch: 22, Loss: 1.09, Accuracy: 0.59\nEpoch: 23, Loss: 1.08, Accuracy: 0.59\nEpoch: 24, Loss: 1.05, Accuracy: 0.60\nEpoch: 25, Loss: 1.05, Accuracy: 0.60\nEpoch: 26, Loss: 1.02, Accuracy: 0.62\nEpoch: 27, Loss: 1.03, Accuracy: 0.61\nEpoch: 28, Loss: 0.99, Accuracy: 0.62\nEpoch: 29, Loss: 0.98, Accuracy: 0.63\nEpoch: 30, Loss: 0.98, Accuracy: 0.64\nEpoch: 31, Loss: 0.96, Accuracy: 0.64\nEpoch: 32, Loss: 0.94, Accuracy: 0.65\nEpoch: 33, Loss: 0.92, Accuracy: 0.66\nEpoch: 34, Loss: 0.91, Accuracy: 0.66\nEpoch: 35, Loss: 0.89, Accuracy: 0.66\nEpoch: 36, Loss: 0.88, Accuracy: 0.66\nEpoch: 37, Loss: 0.86, Accuracy: 0.68\nEpoch: 38, Loss: 0.86, Accuracy: 0.68\nEpoch: 39, Loss: 0.86, Accuracy: 0.67\nEpoch: 40, Loss: 0.84, Accuracy: 0.68\nEpoch: 41, Loss: 0.82, Accuracy: 0.69\nEpoch: 42, Loss: 0.82, Accuracy: 0.69\nEpoch: 43, Loss: 0.81, Accuracy: 0.70\nEpoch: 44, Loss: 0.79, Accuracy: 0.70\nEpoch: 45, Loss: 0.78, Accuracy: 0.70\nEpoch: 46, Loss: 0.77, Accuracy: 0.71\nEpoch: 47, Loss: 0.77, Accuracy: 0.71\nEpoch: 48, Loss: 0.76, Accuracy: 0.71\nEpoch: 49, Loss: 0.75, Accuracy: 0.72\nFinished Training\nAccuracy: 0.63, Total items: 1290\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.metrics import f1_score, roc_auc_score\nimport numpy as np\n\ndef inference(model, val_dl):\n    all_predictions = []\n    all_labels = []\n    \n    # Disable gradient updates\n    with torch.no_grad():\n        for data in val_dl:\n            inputs, labels = data[0].to(device), data[1].to(device)\n\n            # Normalize the inputs\n            inputs_m, inputs_s = inputs.mean(), inputs.std()\n            inputs = (inputs - inputs_m) / inputs_s\n\n            # Get predictions\n            outputs = model(inputs)\n            _, predictions = torch.max(outputs, 1)\n            \n            all_predictions.extend(predictions.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n\n    # Calculate metrics\n    accuracy = np.mean(np.array(all_predictions) == np.array(all_labels))\n    f1 = f1_score(all_labels, all_predictions, average='weighted')\n        \n    print(f'Accuracy: {accuracy:.2f}, F1 Score: {f1:.2f}')\n\n# Run inference on trained model with the validation set\ninference(myModel, val_dl)","metadata":{"execution":{"iopub.status.busy":"2024-06-04T20:09:03.334841Z","iopub.execute_input":"2024-06-04T20:09:03.335112Z","iopub.status.idle":"2024-06-04T20:09:21.222209Z","shell.execute_reply.started":"2024-06-04T20:09:03.335089Z","shell.execute_reply":"2024-06-04T20:09:21.221205Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stdout","text":"Accuracy: 0.65, F1 Score: 0.64\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# MobileVNet","metadata":{}},{"cell_type":"code","source":"import torch\nimport torchvision.models as models\nimport torch.nn as nn\n\n# Load a pretrained MobileNetV3 Large model\nmodel = models.mobilenet_v3_small(pretrained=True)\n\n# Modify the first layer to accept 2-channel input\nfirst_conv_layer = model.features[0][0]\nmodel.features[0][0] = nn.Conv2d(2, first_conv_layer.out_channels,\n                                 kernel_size=first_conv_layer.kernel_size,\n                                 stride=first_conv_layer.stride,\n                                 padding=first_conv_layer.padding)\n\n\nnum_classes = 6\n# Replace the classifier head with a new one adjusted for your number of classes\nmodel.classifier[3] = nn.Linear(model.classifier[3].in_features, num_classes)\n\n# Check if the model should be trained on a GPU\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nmyModel = model.to(device)","metadata":{"execution":{"iopub.status.busy":"2024-06-05T16:14:01.902498Z","iopub.execute_input":"2024-06-05T16:14:01.902993Z","iopub.status.idle":"2024-06-05T16:14:02.825516Z","shell.execute_reply.started":"2024-06-05T16:14:01.902955Z","shell.execute_reply":"2024-06-05T16:14:02.824473Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V3_Small_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V3_Small_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/mobilenet_v3_small-047dcff4.pth\" to /root/.cache/torch/hub/checkpoints/mobilenet_v3_small-047dcff4.pth\n100%|██████████| 9.83M/9.83M [00:00<00:00, 43.6MB/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"# Define training function\ndef training(model, train_dl, num_epochs):\n    criterion = nn.CrossEntropyLoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n    scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.001,\n                                                    steps_per_epoch=int(len(train_dl)),\n                                                    epochs=num_epochs,\n                                                    anneal_strategy='linear')\n\n    for epoch in range(num_epochs):\n        running_loss = 0.0\n        correct_prediction = 0\n        total_prediction = 0\n\n        for i, data in enumerate(train_dl):\n            inputs, labels = data[0].to(device), data[1].to(device)\n\n            inputs_m, inputs_s = inputs.mean(), inputs.std()\n            inputs = (inputs - inputs_m) / inputs_s\n\n            optimizer.zero_grad()\n\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            scheduler.step()\n\n            running_loss += loss.item()\n\n            _, prediction = torch.max(outputs, 1)\n            correct_prediction += (prediction == labels).sum().item()\n            total_prediction += prediction.shape[0]\n\n        avg_loss = running_loss / len(train_dl)\n        acc = correct_prediction / total_prediction\n        print(f'Epoch: {epoch}, Loss: {avg_loss:.2f}, Accuracy: {acc:.2f}')\n\n    print('Finished Training')","metadata":{"execution":{"iopub.status.busy":"2024-06-05T16:14:02.827089Z","iopub.execute_input":"2024-06-05T16:14:02.827359Z","iopub.status.idle":"2024-06-05T16:14:02.837568Z","shell.execute_reply.started":"2024-06-05T16:14:02.827336Z","shell.execute_reply":"2024-06-05T16:14:02.836450Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"# Define inference function\ndef inference(model, val_dl):\n    correct_prediction = 0\n    total_prediction = 0\n\n    with torch.no_grad():\n        for data in val_dl:\n            inputs, labels = data[0].to(device), data[1].to(device)\n\n            inputs_m, inputs_s = inputs.mean(), inputs.std()\n            inputs = (inputs - inputs_m) / inputs_s\n\n            outputs = model(inputs)\n\n            _, prediction = torch.max(outputs, 1)\n            correct_prediction += (prediction == labels).sum().item()\n            total_prediction += prediction.shape[0]\n\n    acc = correct_prediction / total_prediction\n    print(f'Accuracy: {acc:.2f}, Total items: {total_prediction}')","metadata":{"execution":{"iopub.status.busy":"2024-06-05T16:14:02.838564Z","iopub.execute_input":"2024-06-05T16:14:02.838860Z","iopub.status.idle":"2024-06-05T16:14:02.854661Z","shell.execute_reply.started":"2024-06-05T16:14:02.838837Z","shell.execute_reply":"2024-06-05T16:14:02.853701Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"# Train the model\nnum_epochs = 50\ntraining(myModel, train_dl, num_epochs)\n\n# Run inference on the trained model\ninference(myModel, val_dl)","metadata":{"execution":{"iopub.status.busy":"2024-06-05T16:14:02.856234Z","iopub.execute_input":"2024-06-05T16:14:02.856533Z","iopub.status.idle":"2024-06-05T17:33:11.882420Z","shell.execute_reply.started":"2024-06-05T16:14:02.856502Z","shell.execute_reply":"2024-06-05T17:33:11.881309Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"Epoch: 0, Loss: 1.64, Accuracy: 0.32\nEpoch: 1, Loss: 1.50, Accuracy: 0.40\nEpoch: 2, Loss: 1.41, Accuracy: 0.45\nEpoch: 3, Loss: 1.33, Accuracy: 0.49\nEpoch: 4, Loss: 1.28, Accuracy: 0.51\nEpoch: 5, Loss: 1.23, Accuracy: 0.53\nEpoch: 6, Loss: 1.20, Accuracy: 0.54\nEpoch: 7, Loss: 1.16, Accuracy: 0.56\nEpoch: 8, Loss: 1.14, Accuracy: 0.57\nEpoch: 9, Loss: 1.11, Accuracy: 0.59\nEpoch: 10, Loss: 1.10, Accuracy: 0.59\nEpoch: 11, Loss: 1.09, Accuracy: 0.59\nEpoch: 12, Loss: 1.07, Accuracy: 0.60\nEpoch: 13, Loss: 1.05, Accuracy: 0.61\nEpoch: 14, Loss: 1.05, Accuracy: 0.62\nEpoch: 15, Loss: 1.04, Accuracy: 0.62\nEpoch: 16, Loss: 1.03, Accuracy: 0.62\nEpoch: 17, Loss: 0.99, Accuracy: 0.64\nEpoch: 18, Loss: 0.95, Accuracy: 0.65\nEpoch: 19, Loss: 0.95, Accuracy: 0.65\nEpoch: 20, Loss: 0.92, Accuracy: 0.66\nEpoch: 21, Loss: 0.90, Accuracy: 0.67\nEpoch: 22, Loss: 0.86, Accuracy: 0.68\nEpoch: 23, Loss: 0.83, Accuracy: 0.69\nEpoch: 24, Loss: 0.83, Accuracy: 0.69\nEpoch: 25, Loss: 0.78, Accuracy: 0.71\nEpoch: 26, Loss: 0.79, Accuracy: 0.70\nEpoch: 27, Loss: 0.74, Accuracy: 0.72\nEpoch: 28, Loss: 0.74, Accuracy: 0.72\nEpoch: 29, Loss: 0.71, Accuracy: 0.74\nEpoch: 30, Loss: 0.68, Accuracy: 0.74\nEpoch: 31, Loss: 0.64, Accuracy: 0.76\nEpoch: 32, Loss: 0.63, Accuracy: 0.76\nEpoch: 33, Loss: 0.61, Accuracy: 0.77\nEpoch: 34, Loss: 0.61, Accuracy: 0.77\nEpoch: 35, Loss: 0.57, Accuracy: 0.79\nEpoch: 36, Loss: 0.57, Accuracy: 0.79\nEpoch: 37, Loss: 0.54, Accuracy: 0.80\nEpoch: 38, Loss: 0.53, Accuracy: 0.80\nEpoch: 39, Loss: 0.48, Accuracy: 0.82\nEpoch: 40, Loss: 0.47, Accuracy: 0.82\nEpoch: 41, Loss: 0.45, Accuracy: 0.83\nEpoch: 42, Loss: 0.43, Accuracy: 0.84\nEpoch: 43, Loss: 0.42, Accuracy: 0.84\nEpoch: 44, Loss: 0.39, Accuracy: 0.86\nEpoch: 45, Loss: 0.39, Accuracy: 0.86\nEpoch: 46, Loss: 0.37, Accuracy: 0.86\nEpoch: 47, Loss: 0.36, Accuracy: 0.86\nEpoch: 48, Loss: 0.35, Accuracy: 0.87\nEpoch: 49, Loss: 0.35, Accuracy: 0.88\nFinished Training\nAccuracy: 0.68, Total items: 1290\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.metrics import f1_score, roc_auc_score\nimport numpy as np\n\ndef inference(model, val_dl):\n    all_predictions = []\n    all_labels = []\n    \n    # Disable gradient updates\n    with torch.no_grad():\n        for data in val_dl:\n            inputs, labels = data[0].to(device), data[1].to(device)\n\n            # Normalize the inputs\n            inputs_m, inputs_s = inputs.mean(), inputs.std()\n            inputs = (inputs - inputs_m) / inputs_s\n\n            # Get predictions\n            outputs = model(inputs)\n            _, predictions = torch.max(outputs, 1)\n            \n            all_predictions.extend(predictions.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n\n    # Calculate metrics\n    accuracy = np.mean(np.array(all_predictions) == np.array(all_labels))\n    f1 = f1_score(all_labels, all_predictions, average='weighted')\n        \n    print(f'Accuracy: {accuracy:.2f}, F1 Score: {f1:.2f}')\n\n# Run inference on trained model with the validation set\ninference(myModel, val_dl)","metadata":{"execution":{"iopub.status.busy":"2024-06-05T17:33:11.883821Z","iopub.execute_input":"2024-06-05T17:33:11.884174Z","iopub.status.idle":"2024-06-05T17:33:36.725669Z","shell.execute_reply.started":"2024-06-05T17:33:11.884136Z","shell.execute_reply":"2024-06-05T17:33:36.724587Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"Accuracy: 0.69, F1 Score: 0.69\n","output_type":"stream"}]}]}