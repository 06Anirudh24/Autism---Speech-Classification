{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7405399,"sourceType":"datasetVersion","datasetId":4306525},{"sourceId":8588058,"sourceType":"datasetVersion","datasetId":5136800}],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras import datasets, layers, models\nimport matplotlib.pyplot as plt\n\nimport pandas as pd\nimport math, random\nimport torch\nimport torchaudio\nfrom torchaudio import transforms\nfrom IPython.display import Audio\n\nfrom torch.utils.data import DataLoader, Dataset, random_split\nimport torch.nn.functional as F\nfrom torch.nn import init\nimport torch\nimport torchvision\nimport torch.nn as nn\n\n\n\nimport torchvision.models as models\nfrom torchvision import transforms as T","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-06-24T11:44:16.771786Z","iopub.execute_input":"2024-06-24T11:44:16.772123Z","iopub.status.idle":"2024-06-24T11:44:34.405485Z","shell.execute_reply.started":"2024-06-24T11:44:16.772096Z","shell.execute_reply":"2024-06-24T11:44:34.404503Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"2024-06-24 11:44:18.585367: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-06-24 11:44:18.585487: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-06-24 11:44:18.718250: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"import timm","metadata":{"execution":{"iopub.status.busy":"2024-06-24T11:44:34.407184Z","iopub.execute_input":"2024-06-24T11:44:34.407667Z","iopub.status.idle":"2024-06-24T11:44:35.296367Z","shell.execute_reply.started":"2024-06-24T11:44:34.407641Z","shell.execute_reply":"2024-06-24T11:44:35.295380Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"path = \"/kaggle/input/autism-paper-data/Retained/dataset_file_directory.csv\"\ndf_original = pd.read_csv(path)\ndf_original.head()\n","metadata":{"execution":{"iopub.status.busy":"2024-06-24T11:44:35.297491Z","iopub.execute_input":"2024-06-24T11:44:35.297785Z","iopub.status.idle":"2024-06-24T11:44:35.339098Z","shell.execute_reply.started":"2024-06-24T11:44:35.297751Z","shell.execute_reply":"2024-06-24T11:44:35.338222Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"                                    Filename Participant   Label\n0   210408_2025_00-01-38.27--00-01-40.84.wav         P01  social\n1   210324_2036_00-06-03.61--00-06-05.78.wav         P01  social\n2   210324_2036_00-09-04.66--00-09-06.16.wav         P01  social\n3     210324_2036_00-11-18.6--00-11-20.3.wav         P01  social\n4  200506_2110_00-01-25.92--00-01-26.58c.wav         P01  social","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Filename</th>\n      <th>Participant</th>\n      <th>Label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>210408_2025_00-01-38.27--00-01-40.84.wav</td>\n      <td>P01</td>\n      <td>social</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>210324_2036_00-06-03.61--00-06-05.78.wav</td>\n      <td>P01</td>\n      <td>social</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>210324_2036_00-09-04.66--00-09-06.16.wav</td>\n      <td>P01</td>\n      <td>social</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>210324_2036_00-11-18.6--00-11-20.3.wav</td>\n      <td>P01</td>\n      <td>social</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>200506_2110_00-01-25.92--00-01-26.58c.wav</td>\n      <td>P01</td>\n      <td>social</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"df_original.shape[0]","metadata":{"execution":{"iopub.status.busy":"2024-06-24T11:44:35.340056Z","iopub.execute_input":"2024-06-24T11:44:35.340313Z","iopub.status.idle":"2024-06-24T11:44:35.345984Z","shell.execute_reply.started":"2024-06-24T11:44:35.340290Z","shell.execute_reply":"2024-06-24T11:44:35.345058Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"6450"},"metadata":{}}]},{"cell_type":"code","source":"df_original.Label.unique() #Print unique types of sound","metadata":{"execution":{"iopub.status.busy":"2024-06-24T11:44:35.349043Z","iopub.execute_input":"2024-06-24T11:44:35.349683Z","iopub.status.idle":"2024-06-24T11:44:35.358461Z","shell.execute_reply.started":"2024-06-24T11:44:35.349650Z","shell.execute_reply":"2024-06-24T11:44:35.357471Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"array(['social', 'selftalk', 'request', 'delighted', 'dysregulated',\n       'frustrated'], dtype=object)"},"metadata":{}}]},{"cell_type":"code","source":"df_original['Label'].value_counts() #Number of datapoints available for each target variable","metadata":{"execution":{"iopub.status.busy":"2024-06-24T11:44:35.359546Z","iopub.execute_input":"2024-06-24T11:44:35.359895Z","iopub.status.idle":"2024-06-24T11:44:35.374128Z","shell.execute_reply.started":"2024-06-24T11:44:35.359861Z","shell.execute_reply":"2024-06-24T11:44:35.373262Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"Label\nselftalk        1885\nfrustrated      1536\ndelighted       1272\ndysregulated     704\nsocial           634\nrequest          419\nName: count, dtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"df_original[\"classID\"] = df_original['Label']\ndf_original['classID'].replace(['selftalk', 'frustrated', 'delighted', 'dysregulated', 'social', 'request'],\n                        [0, 1, 2, 3, 4, 5], inplace=True)\n","metadata":{"execution":{"iopub.status.busy":"2024-06-24T11:44:35.375261Z","iopub.execute_input":"2024-06-24T11:44:35.375539Z","iopub.status.idle":"2024-06-24T11:44:35.389066Z","shell.execute_reply.started":"2024-06-24T11:44:35.375515Z","shell.execute_reply":"2024-06-24T11:44:35.388109Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_34/1495406677.py:2: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\nThe behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n\nFor example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n\n\n  df_original['classID'].replace(['selftalk', 'frustrated', 'delighted', 'dysregulated', 'social', 'request'],\n/tmp/ipykernel_34/1495406677.py:2: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n  df_original['classID'].replace(['selftalk', 'frustrated', 'delighted', 'dysregulated', 'social', 'request'],\n","output_type":"stream"}]},{"cell_type":"code","source":"df = df_original\ndf = df.drop(['Participant', 'Label'], axis=1)\ndf.head()\n","metadata":{"execution":{"iopub.status.busy":"2024-06-24T11:44:35.390157Z","iopub.execute_input":"2024-06-24T11:44:35.390413Z","iopub.status.idle":"2024-06-24T11:44:35.405704Z","shell.execute_reply.started":"2024-06-24T11:44:35.390391Z","shell.execute_reply":"2024-06-24T11:44:35.404873Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"                                    Filename  classID\n0   210408_2025_00-01-38.27--00-01-40.84.wav        4\n1   210324_2036_00-06-03.61--00-06-05.78.wav        4\n2   210324_2036_00-09-04.66--00-09-06.16.wav        4\n3     210324_2036_00-11-18.6--00-11-20.3.wav        4\n4  200506_2110_00-01-25.92--00-01-26.58c.wav        4","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Filename</th>\n      <th>classID</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>210408_2025_00-01-38.27--00-01-40.84.wav</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>210324_2036_00-06-03.61--00-06-05.78.wav</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>210324_2036_00-09-04.66--00-09-06.16.wav</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>210324_2036_00-11-18.6--00-11-20.3.wav</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>200506_2110_00-01-25.92--00-01-26.58c.wav</td>\n      <td>4</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"df['classID'].value_counts() #Number of datapoints available for each target variable","metadata":{"execution":{"iopub.status.busy":"2024-06-24T11:44:35.406703Z","iopub.execute_input":"2024-06-24T11:44:35.406982Z","iopub.status.idle":"2024-06-24T11:44:35.422467Z","shell.execute_reply.started":"2024-06-24T11:44:35.406951Z","shell.execute_reply":"2024-06-24T11:44:35.421648Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"classID\n0    1885\n1    1536\n2    1272\n3     704\n4     634\n5     419\nName: count, dtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"# Construct file path by concatenating fold and file name\ndf['relative_path'] = '/' + df['Filename'].astype(str)\n\n# Take relevant columns\ndf = df[['relative_path', 'classID']]\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2024-06-24T11:44:35.423687Z","iopub.execute_input":"2024-06-24T11:44:35.423992Z","iopub.status.idle":"2024-06-24T11:44:35.443273Z","shell.execute_reply.started":"2024-06-24T11:44:35.423968Z","shell.execute_reply":"2024-06-24T11:44:35.442411Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"                                relative_path  classID\n0   /210408_2025_00-01-38.27--00-01-40.84.wav        4\n1   /210324_2036_00-06-03.61--00-06-05.78.wav        4\n2   /210324_2036_00-09-04.66--00-09-06.16.wav        4\n3     /210324_2036_00-11-18.6--00-11-20.3.wav        4\n4  /200506_2110_00-01-25.92--00-01-26.58c.wav        4","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>relative_path</th>\n      <th>classID</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>/210408_2025_00-01-38.27--00-01-40.84.wav</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>/210324_2036_00-06-03.61--00-06-05.78.wav</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>/210324_2036_00-09-04.66--00-09-06.16.wav</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>/210324_2036_00-11-18.6--00-11-20.3.wav</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>/200506_2110_00-01-25.92--00-01-26.58c.wav</td>\n      <td>4</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"#Random shuffle of rows.\ndf = df. sample(frac=1)\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2024-06-24T11:44:35.444385Z","iopub.execute_input":"2024-06-24T11:44:35.445022Z","iopub.status.idle":"2024-06-24T11:44:35.457806Z","shell.execute_reply.started":"2024-06-24T11:44:35.444991Z","shell.execute_reply":"2024-06-24T11:44:35.456869Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"                                    relative_path  classID\n2945   /200929_0101_00-00-26.37--00-00-27.40c.wav        0\n7       /200506_2110_00-02-55.52--00-02-58.79.wav        4\n6125    /210113_1909_00-03-55.66--00-03-56.11.wav        1\n1196     /200229_2156_00-03-55.9--00-03-56.29.wav        2\n4422  /200906_1111_00-00-25.06--00-00-25.774c.wav        4","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>relative_path</th>\n      <th>classID</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2945</th>\n      <td>/200929_0101_00-00-26.37--00-00-27.40c.wav</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>/200506_2110_00-02-55.52--00-02-58.79.wav</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>6125</th>\n      <td>/210113_1909_00-03-55.66--00-03-56.11.wav</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1196</th>\n      <td>/200229_2156_00-03-55.9--00-03-56.29.wav</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>4422</th>\n      <td>/200906_1111_00-00-25.06--00-00-25.774c.wav</td>\n      <td>4</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"\n# Audio utility functions\nclass AudioUtil():\n    @staticmethod\n    def open(audio_file):\n        sig, sr = torchaudio.load(audio_file)\n        return (sig, sr)\n\n    @staticmethod\n    def rechannel(aud, new_channel):\n        sig, sr = aud\n        if sig.shape[0] == new_channel:\n            return aud\n        if new_channel == 1:\n            resig = sig[:1, :]\n        else:\n            resig = torch.cat([sig, sig])\n        return (resig, sr)\n\n    @staticmethod\n    def resample(aud, newsr):\n        sig, sr = aud\n        if sr == newsr:\n            return aud\n        num_channels = sig.shape[0]\n        resig = torchaudio.transforms.Resample(sr, newsr)(sig[:1, :])\n        if num_channels > 1:\n            retwo = torchaudio.transforms.Resample(sr, newsr)(sig[1:, :])\n            resig = torch.cat([resig, retwo])\n        return (resig, newsr)\n\n    @staticmethod\n    def pad_trunc(aud, max_ms):\n        sig, sr = aud\n        num_rows, sig_len = sig.shape\n        max_len = sr // 1000 * max_ms\n        if sig_len > max_len:\n            sig = sig[:, :max_len]\n        elif sig_len < max_len:\n            pad_begin_len = random.randint(0, max_len - sig_len)\n            pad_end_len = max_len - sig_len - pad_begin_len\n            pad_begin = torch.zeros((num_rows, pad_begin_len))\n            pad_end = torch.zeros((num_rows, pad_end_len))\n            sig = torch.cat((pad_begin, sig, pad_end), 1)\n        return (sig, sr)\n\n    @staticmethod\n    def time_shift(aud, shift_limit):\n        sig, sr = aud\n        _, sig_len = sig.shape\n        shift_amt = int(random.random() * shift_limit * sig_len)\n        return (sig.roll(shift_amt), sr)\n\n    @staticmethod\n    def spectro_gram(aud, n_mels=64, n_fft=1024, hop_len=None):\n        sig, sr = aud\n        top_db = 80\n        spec = transforms.MelSpectrogram(sr, n_fft=n_fft, hop_length=hop_len, n_mels=n_mels)(sig)\n        spec = transforms.AmplitudeToDB(top_db=top_db)(spec)\n        return (spec)\n\n    @staticmethod\n    def spectro_augment(spec, max_mask_pct=0.1, n_freq_masks=1, n_time_masks=1):\n        _, n_mels, n_steps = spec.shape\n        mask_value = spec.mean()\n        aug_spec = spec\n        freq_mask_param = max_mask_pct * n_mels\n        for _ in range(n_freq_masks):\n            aug_spec = transforms.FrequencyMasking(freq_mask_param)(aug_spec, mask_value)\n        time_mask_param = max_mask_pct * n_steps\n        for _ in range(n_time_masks):\n            aug_spec = transforms.TimeMasking(time_mask_param)(aug_spec, mask_value)\n        return aug_spec\n","metadata":{"execution":{"iopub.status.busy":"2024-06-24T11:44:35.459172Z","iopub.execute_input":"2024-06-24T11:44:35.459483Z","iopub.status.idle":"2024-06-24T11:44:35.475243Z","shell.execute_reply.started":"2024-06-24T11:44:35.459449Z","shell.execute_reply":"2024-06-24T11:44:35.474360Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"import torchvision\n","metadata":{"execution":{"iopub.status.busy":"2024-06-24T11:44:35.476252Z","iopub.execute_input":"2024-06-24T11:44:35.476502Z","iopub.status.idle":"2024-06-24T11:44:35.487587Z","shell.execute_reply.started":"2024-06-24T11:44:35.476463Z","shell.execute_reply":"2024-06-24T11:44:35.486774Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Sound dataset class\nclass SoundDS(Dataset):\n    def __init__(self, df, data_path):\n        self.df = df\n        self.data_path = str(data_path)\n        self.duration = 4000\n        self.sr = 44100\n        self.channel = 2\n        self.shift_pct = 0.4\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        audio_file = self.data_path + self.df.loc[idx, 'relative_path']\n        class_id = self.df.loc[idx, 'classID']\n        aud = AudioUtil.open(audio_file)\n        reaud = AudioUtil.resample(aud, self.sr)\n        rechan = AudioUtil.rechannel(reaud, self.channel)\n        dur_aud = AudioUtil.pad_trunc(rechan, self.duration)\n        shift_aud = AudioUtil.time_shift(dur_aud, self.shift_pct)\n        sgram = AudioUtil.spectro_gram(shift_aud, n_mels=64, n_fft=1024, hop_len=None)\n        aug_sgram = AudioUtil.spectro_augment(sgram, max_mask_pct=0.1, n_freq_masks=2, n_time_masks=2)\n        aug_sgram = torchvision.transforms.Resize((224, 224))(aug_sgram)\n        return aug_sgram, class_id\n","metadata":{"execution":{"iopub.status.busy":"2024-06-24T11:44:35.491212Z","iopub.execute_input":"2024-06-24T11:44:35.491489Z","iopub.status.idle":"2024-06-24T11:44:35.500528Z","shell.execute_reply.started":"2024-06-24T11:44:35.491468Z","shell.execute_reply":"2024-06-24T11:44:35.499753Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"data_path = \"/kaggle/input/autism-paper-data/Retained/audio\"\nmyds = SoundDS(df, data_path)\n#myds = SoundDS(df, path)\n\n#Random split of 80:20 between training and validation\nnum_items = len(myds)\nnum_train = round(num_items * 0.8)\nnum_val = num_items - num_train\ntrain_ds, val_ds = random_split(myds, [num_train, num_val])\n\n#Create training and validation data loaders\ntrain_dl = torch.utils.data.DataLoader(train_ds, batch_size=16, shuffle=True)\nval_dl = torch.utils.data.DataLoader(val_ds, batch_size=16, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2024-06-24T11:44:35.501602Z","iopub.execute_input":"2024-06-24T11:44:35.502035Z","iopub.status.idle":"2024-06-24T11:44:35.524455Z","shell.execute_reply.started":"2024-06-24T11:44:35.502012Z","shell.execute_reply":"2024-06-24T11:44:35.523590Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"print(num_items, num_train, num_val)","metadata":{"execution":{"iopub.status.busy":"2024-06-24T11:44:35.525524Z","iopub.execute_input":"2024-06-24T11:44:35.525869Z","iopub.status.idle":"2024-06-24T11:44:35.530735Z","shell.execute_reply.started":"2024-06-24T11:44:35.525821Z","shell.execute_reply":"2024-06-24T11:44:35.529811Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"6450 5160 1290\n","output_type":"stream"}]},{"cell_type":"code","source":"type(train_dl)","metadata":{"execution":{"iopub.status.busy":"2024-06-24T11:44:35.531805Z","iopub.execute_input":"2024-06-24T11:44:35.532077Z","iopub.status.idle":"2024-06-24T11:44:35.540871Z","shell.execute_reply.started":"2024-06-24T11:44:35.532055Z","shell.execute_reply":"2024-06-24T11:44:35.540050Z"},"trusted":true},"execution_count":17,"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"torch.utils.data.dataloader.DataLoader"},"metadata":{}}]},{"cell_type":"code","source":"from torchvision import models","metadata":{"execution":{"iopub.status.busy":"2024-06-24T11:44:35.541867Z","iopub.execute_input":"2024-06-24T11:44:35.542112Z","iopub.status.idle":"2024-06-24T11:44:35.549325Z","shell.execute_reply.started":"2024-06-24T11:44:35.542091Z","shell.execute_reply":"2024-06-24T11:44:35.548395Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"# Load and modify MobileNetV2 model\nmodel = torchvision.models.mobilenet_v2(pretrained=True)\nmodel.features[0][0] = nn.Conv2d(2, model.features[0][0].out_channels,\n                                  kernel_size=model.features[0][0].kernel_size,\n                                  stride=model.features[0][0].stride,\n                                  padding=model.features[0][0].padding)\nnum_ftrs = model.classifier[1].in_features\nmodel.classifier[1] = nn.Linear(num_ftrs, df_original['classID'].nunique())\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nmyModel = model.to(device)\n\n# Check that it is on Cuda\nnext(myModel.parameters()).device","metadata":{"execution":{"iopub.status.busy":"2024-06-24T11:44:35.550597Z","iopub.execute_input":"2024-06-24T11:44:35.550888Z","iopub.status.idle":"2024-06-24T11:44:36.058172Z","shell.execute_reply.started":"2024-06-24T11:44:35.550861Z","shell.execute_reply":"2024-06-24T11:44:36.057262Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V2_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V2_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/mobilenet_v2-b0353104.pth\" to /root/.cache/torch/hub/checkpoints/mobilenet_v2-b0353104.pth\n100%|██████████| 13.6M/13.6M [00:00<00:00, 149MB/s]\n","output_type":"stream"},{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"device(type='cuda', index=0)"},"metadata":{}}]},{"cell_type":"code","source":"def training(model, train_dl, num_epochs):\n    criterion = nn.CrossEntropyLoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n    scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.001,\n                                                    steps_per_epoch=int(len(train_dl)),\n                                                    epochs=num_epochs,\n                                                    anneal_strategy='linear')\n\n    for epoch in range(num_epochs):\n        running_loss = 0.0\n        correct_prediction = 0\n        total_prediction = 0\n\n        for i, data in enumerate(train_dl):\n            inputs, labels = data[0].to(device), data[1].to(device)\n            inputs_m, inputs_s = inputs.mean(), inputs.std()\n            inputs = (inputs - inputs_m) / inputs_s\n\n            optimizer.zero_grad()\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            scheduler.step()\n\n            running_loss += loss.item()\n            _, prediction = torch.max(outputs, 1)\n            correct_prediction += (prediction == labels).sum().item()\n            total_prediction += prediction.shape[0]\n\n        num_batches = len(train_dl)\n        avg_loss = running_loss / num_batches\n        acc = correct_prediction / total_prediction\n        print(f'Epoch: {epoch}, Loss: {avg_loss:.2f}, Accuracy: {acc:.2f}')\n\n    print('Finished Training')","metadata":{"execution":{"iopub.status.busy":"2024-06-24T11:44:36.059353Z","iopub.execute_input":"2024-06-24T11:44:36.059643Z","iopub.status.idle":"2024-06-24T11:44:36.069266Z","shell.execute_reply.started":"2024-06-24T11:44:36.059618Z","shell.execute_reply":"2024-06-24T11:44:36.068391Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"def inference(model, val_dl):\n    correct_prediction = 0\n    total_prediction = 0\n\n    with torch.no_grad():\n        for data in val_dl:\n            inputs, labels = data[0].to(device), data[1].to(device)\n            inputs_m, inputs_s = inputs.mean(), inputs.std()\n            inputs = (inputs - inputs_m) / inputs_s\n\n            outputs = model(inputs)\n            _, prediction = torch.max(outputs, 1)\n            correct_prediction += (prediction == labels).sum().item()\n            total_prediction += prediction.shape[0]\n\n    acc = correct_prediction / total_prediction\n    print(f'Accuracy: {acc:.2f}, Total items: {total_prediction}')\n","metadata":{"execution":{"iopub.status.busy":"2024-06-24T11:44:36.070664Z","iopub.execute_input":"2024-06-24T11:44:36.070946Z","iopub.status.idle":"2024-06-24T11:44:36.081549Z","shell.execute_reply.started":"2024-06-24T11:44:36.070923Z","shell.execute_reply":"2024-06-24T11:44:36.080636Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"print(next(myModel.parameters()).device)","metadata":{"execution":{"iopub.status.busy":"2024-06-24T11:44:36.082749Z","iopub.execute_input":"2024-06-24T11:44:36.083318Z","iopub.status.idle":"2024-06-24T11:44:36.093362Z","shell.execute_reply.started":"2024-06-24T11:44:36.083286Z","shell.execute_reply":"2024-06-24T11:44:36.092476Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"cuda:0\n","output_type":"stream"}]},{"cell_type":"code","source":"num_epochs = 50\ntraining(myModel, train_dl, num_epochs)\ninference(myModel, val_dl)","metadata":{"execution":{"iopub.status.busy":"2024-06-24T11:44:36.094334Z","iopub.execute_input":"2024-06-24T11:44:36.094580Z","iopub.status.idle":"2024-06-24T13:04:18.296816Z","shell.execute_reply.started":"2024-06-24T11:44:36.094557Z","shell.execute_reply":"2024-06-24T13:04:18.295618Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 0, Loss: 1.58, Accuracy: 0.35\nEpoch: 1, Loss: 1.41, Accuracy: 0.44\nEpoch: 2, Loss: 1.33, Accuracy: 0.48\nEpoch: 3, Loss: 1.26, Accuracy: 0.53\nEpoch: 4, Loss: 1.21, Accuracy: 0.55\nEpoch: 5, Loss: 1.18, Accuracy: 0.57\nEpoch: 6, Loss: 1.16, Accuracy: 0.57\nEpoch: 7, Loss: 1.12, Accuracy: 0.59\nEpoch: 8, Loss: 1.08, Accuracy: 0.60\nEpoch: 9, Loss: 1.06, Accuracy: 0.61\nEpoch: 10, Loss: 1.05, Accuracy: 0.61\nEpoch: 11, Loss: 1.03, Accuracy: 0.63\nEpoch: 12, Loss: 1.03, Accuracy: 0.64\nEpoch: 13, Loss: 1.02, Accuracy: 0.62\nEpoch: 14, Loss: 1.02, Accuracy: 0.64\nEpoch: 15, Loss: 1.00, Accuracy: 0.63\nEpoch: 16, Loss: 0.97, Accuracy: 0.65\nEpoch: 17, Loss: 0.94, Accuracy: 0.65\nEpoch: 18, Loss: 0.90, Accuracy: 0.67\nEpoch: 19, Loss: 0.88, Accuracy: 0.68\nEpoch: 20, Loss: 0.87, Accuracy: 0.69\nEpoch: 21, Loss: 0.82, Accuracy: 0.71\nEpoch: 22, Loss: 0.80, Accuracy: 0.71\nEpoch: 23, Loss: 0.78, Accuracy: 0.71\nEpoch: 24, Loss: 0.74, Accuracy: 0.73\nEpoch: 25, Loss: 0.73, Accuracy: 0.74\nEpoch: 26, Loss: 0.69, Accuracy: 0.75\nEpoch: 27, Loss: 0.67, Accuracy: 0.75\nEpoch: 28, Loss: 0.63, Accuracy: 0.77\nEpoch: 29, Loss: 0.63, Accuracy: 0.77\nEpoch: 30, Loss: 0.58, Accuracy: 0.78\nEpoch: 31, Loss: 0.56, Accuracy: 0.79\nEpoch: 32, Loss: 0.53, Accuracy: 0.81\nEpoch: 33, Loss: 0.50, Accuracy: 0.82\nEpoch: 34, Loss: 0.46, Accuracy: 0.83\nEpoch: 35, Loss: 0.44, Accuracy: 0.84\nEpoch: 36, Loss: 0.42, Accuracy: 0.85\nEpoch: 37, Loss: 0.39, Accuracy: 0.86\nEpoch: 38, Loss: 0.37, Accuracy: 0.87\nEpoch: 39, Loss: 0.34, Accuracy: 0.88\nEpoch: 40, Loss: 0.31, Accuracy: 0.89\nEpoch: 41, Loss: 0.29, Accuracy: 0.89\nEpoch: 42, Loss: 0.26, Accuracy: 0.91\nEpoch: 43, Loss: 0.25, Accuracy: 0.91\nEpoch: 44, Loss: 0.24, Accuracy: 0.92\nEpoch: 45, Loss: 0.22, Accuracy: 0.92\nEpoch: 46, Loss: 0.19, Accuracy: 0.93\nEpoch: 47, Loss: 0.19, Accuracy: 0.93\nEpoch: 48, Loss: 0.17, Accuracy: 0.94\nEpoch: 49, Loss: 0.17, Accuracy: 0.94\nFinished Training\nAccuracy: 0.71, Total items: 1290\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.metrics import f1_score, roc_auc_score\nimport numpy as np\n\ndef inference(model, val_dl):\n    all_predictions = []\n    all_labels = []\n    \n    # Disable gradient updates\n    with torch.no_grad():\n        for data in val_dl:\n            inputs, labels = data[0].to(device), data[1].to(device)\n\n            # Normalize the inputs\n            inputs_m, inputs_s = inputs.mean(), inputs.std()\n            inputs = (inputs - inputs_m) / inputs_s\n\n            # Get predictions\n            outputs = model(inputs)\n            _, predictions = torch.max(outputs, 1)\n            \n            all_predictions.extend(predictions.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n\n    # Calculate metrics\n    accuracy = np.mean(np.array(all_predictions) == np.array(all_labels))\n    f1 = f1_score(all_labels, all_predictions, average='weighted')\n        \n    print(f'Accuracy: {accuracy:.2f}, F1 Score: {f1:.2f}')\n\n# Run inference on trained model with the validation set\ninference(myModel, val_dl)","metadata":{"execution":{"iopub.status.busy":"2024-06-24T13:04:18.298159Z","iopub.execute_input":"2024-06-24T13:04:18.298517Z","iopub.status.idle":"2024-06-24T13:04:39.932574Z","shell.execute_reply.started":"2024-06-24T13:04:18.298485Z","shell.execute_reply":"2024-06-24T13:04:39.931570Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Accuracy: 0.71, F1 Score: 0.71\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}